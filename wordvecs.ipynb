{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var csc = IPython.keyboard_manager.command_shortcuts\n",
    "csc.add_shortcut('Ctrl-k','ipython.move-selected-cell-up')\n",
    "csc.add_shortcut('Ctrl-j','ipython.move-selected-cell-down')\n",
    "csc.add_shortcut('Shift-m','ipython.merge-selected-cell-with-cell-after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from project_imports import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "cachedir = 'cache/'\n",
    "memory = Memory(cachedir=cachedir, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import utils as ut; reload(ut);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bksall = ut.BookSeries(7)\n",
    "all_text = bksall.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Word-Vectors](#Word-Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "%time nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bktksall = {i: nlp(bktxt, tag=True, parse=True, entity=True)\n",
    "            for i, bktxt in bksall.txts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time atks = nlp(bksall.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get named entity phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phrase_ents = Series(Counter([e.orth_ for e in ltoks.ents if e.label in ent_nums and ' ' in e.orth_])).sort_values(ascending=0)\n",
    "ent_cands = phrase_ents[phrase_ents > 2].sort_index()\n",
    "ent_cands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ent_cnt = Series(Counter([(e.label_, e.orth_) for e in atks.ents]))\n",
    "ent_cnt.reset_index(drop=0).to_csv('/tmp/ents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ent_lab2num = {e.label_: e.label for e in atks.ents}\n",
    "ent_nums = sorted({ent_lab2num[lab] for lab in 'EVENT FAC GPE ORG PERSON WORK_OF_ART'.split()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "phrase_ents_low = (Series(Counter(\n",
    "            [e.orth_.lower() for e in atks.ents if e.label in ent_nums\n",
    "             and ' ' in e.orth_])).sort_values(ascending=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "phrase_ents = (Series(Counter(\n",
    "            [e.orth_ for e in atks.ents if e.label in ent_nums\n",
    "             and ' ' in e.orth_])).sort_values(ascending=0)).reset_index(drop=0)\n",
    "phrase_ents.columns = ['Phrase', 'Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "phrase_ents2 = (Series(Counter(\n",
    "            [e.orth_ for e in atks.ents if e.label in ent_nums\n",
    "             and ' ' in e.orth_])).sort_values(ascending=0)).reset_index(drop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rep_phrase(s):\n",
    "    \"\"\"Pass string as a phrase to replace with words joined by\n",
    "    underscores. If returns None, then don't replace it. Else,\n",
    "    the returned string is ok to replace.\n",
    "    >>> rep_phrase('the Harry Potter') == 'Harry Potter'\n",
    "    \"\"\"\n",
    "    if isinstance(s, str):\n",
    "        wds = s.split()\n",
    "    else:\n",
    "        wds = s\n",
    "    if len(wds) < 2:\n",
    "        return\n",
    "    if wds[0][0].islower():\n",
    "        return rep_phrase(wds[1:])\n",
    "    if wds[-1][0].islower():\n",
    "        return rep_phrase(wds[:-1])\n",
    "    return ' '.join(wds)\n",
    "    \n",
    "    \n",
    "def clean_hyphen(s):\n",
    "    \"Keep hyphens that separate words, discard rest\"\n",
    "    surr_hyph_re = re.compile(r\"(?<=[A-Za-z])-(?=[A-Za-z])\")\n",
    "    sentinel = b'\\uF5DC'.decode('unicode_escape')\n",
    "    s = surr_hyph_re.sub(sentinel, s)\n",
    "    s = re.sub('-', '', s)\n",
    "    s = re.sub(sentinel, '-', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def tokenize(s: str=bksall.txt):\n",
    "    split_re = re.compile(r\"[^A-Za-z-\\d_']\")\n",
    "    return np.array(filter(bool, [t.strip(\"'\") for t in split_re.split(clean_hyphen(s))]))\n",
    "\n",
    "\n",
    "def multi_replace(pairs, text):\n",
    "    return reduce(lambda accum, x: accum.replace(x[0], x[1]),  pairs, text)\n",
    "\n",
    "\n",
    "def multi_replace(dct, text):\n",
    "    \"\"\"Replace occurrence of keys in `text` with corresponding values,\n",
    "    longest keys first\"\"\"\n",
    "    pairs = sorted(dct.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    return reduce(lambda accum, x: accum.replace(x[0], x[1]), pairs, text)\n",
    "\n",
    "# test_rep_phrase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_rep_phrase():\n",
    "    assert rep_phrase('Harry Potter') == 'Harry Potter'\n",
    "    assert rep_phrase('the Harry Potter') == 'Harry Potter'\n",
    "    assert rep_phrase('Harry Potter.') == 'Harry Potter.'\n",
    "    assert rep_phrase(\"Harry Potter's toy\") == \"Harry Potter's\"\n",
    "    assert rep_phrase(\"the Potter\") is None\n",
    "    \n",
    "def test_tokenize():\n",
    "    assert clean_hyphen('whoops-a-daisy') == 'whoops-a-daisy'\n",
    "    assert clean_hyphen('whoops- a-daisy') == 'whoops a-daisy'\n",
    "    assert clean_hyphen('whoops-a -daisy') == 'whoops-a daisy'\n",
    "    assert clean_hyphen('whoops-a -daisy') == 'whoops-a daisy'\n",
    "    assert clean_hyphen(' -a a- -a- a-a a-a-a ') == ' a a a a-a a-a-a '\n",
    "    assert all(tokenize('whoops-a-daisy, -there- -it goes-') == ['whoops-a-daisy', 'there', 'it', 'goes'])\n",
    "    \n",
    "test_rep_phrase()\n",
    "test_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases = (phrase_ents.assign(Clean=lambda x: x.Phrase.map(clean_hyphen))\n",
    "           .query('Count >= 7 & Clean == Phrase').Phrase)  # .map(rep_phrase).dropna()\n",
    "phrase2wds = OrderedDict([(w, ut.phrase2wd(w)) for w in phrases])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extras2 = [h for i, h in enumerate(hss2) if h not in s1]\n",
    "print(len(extras2))\n",
    "extras2\n",
    "extras1 = [h for i, h in enumerate(hss) if h not in s2]\n",
    "print(len(extras1))\n",
    "extras1\n",
    "a2 = clean_hyphen(all_text)\n",
    "hs2 = hyph_re.findall(a2)\n",
    "hss2 = filter(surr_hyph_re.findall, hs2)\n",
    "\n",
    "phrase_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "phrase_ents\n",
    "phrase_ents[phrase_ents.Phrase.str.startswith('Pro')]\n",
    "n = 1040000\n",
    "' '.join(toks[n:n+1000])\n",
    "n = 5009000\n",
    "print(phrased_text[n:n+9500])\n",
    "phrase_ents.value_counts(normalize=0)\n",
    "\n",
    "phrase_ents[phrase_ents ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bigram Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_big_scores(big_dct, uni_dct, δ=2):\n",
    "#     global w1, w2\n",
    "#     return {(w1, w2): (cts - δ) / ((uni_dct[w1] or print('w1', w1)) *\n",
    "#                          (uni_dct[w2] or print('w2', w2)))\n",
    "#             for w1, w1dct in big_dct.items()\n",
    "#             for w2, cts in w1dct.items()}\n",
    "\n",
    "def get_big_scores(big_dct, uni_dct, δ=2):\n",
    "    #global w1, w2\n",
    "    dct = {}\n",
    "    for w1, w1dct in big_dct.items():\n",
    "        for w2, cts in w1dct.items():\n",
    "            assert uni_dct[w1] or uni_dct[w2], (\"Some word isn't \"\n",
    "                                                \"accounted for in unigrams\")\n",
    "            dct[(w1, w2)] = (cts - δ) / (uni_dct[w1] * uni_dct[w2])\n",
    "    return dct\n",
    "\n",
    "def get_bigrams(toks, δ=9):\n",
    "    bigd = defaultdict(lambda: defaultdict(int))\n",
    "    for w1, w2 in builtins.zip(toks, toks[1:]):\n",
    "        if (w1 == 'va') or (w2 == 'va'):\n",
    "            print('w1', w1)\n",
    "            print('w2', w2)\n",
    "        bigd[w1][w2] += 1\n",
    "\n",
    "    unid = defaultdict(int)\n",
    "    unid.update({k: sum(v.values()) for k, v in bigd.items()}.items() | {toks[-1]: 1}.items())\n",
    "    \n",
    "    big_scores = get_big_scores(bigd, unid, δ=δ)\n",
    "    \n",
    "    bigdf = (Series(big_scores).reset_index(drop=0).sort_values(0, ascending=0)\n",
    "     .rename(columns={'level_0': 'W1', 'level_1': 'W2', 0: 'Score'})\n",
    "     .reset_index(drop=1)\n",
    "    )\n",
    "    \n",
    "    minerva = bigdf.query('W1 == \"Professor\" & W2 == \"McGonagall\"').Score.iloc[0]\n",
    "    bigdf = bigdf.query('Score >= @minerva')\n",
    "    \n",
    "    bigdf['From'] = bigdf.W1 + ' ' + bigdf.W2\n",
    "    bigdf['From'] = bigdf.W1 + '_' + bigdf.W2\n",
    "    return bigdf[bigdf.W1.map(iscap) & bigdf.W2.map(iscap)\n",
    "                 & ~bigdf.W1.str.isupper() & ~bigdf.W2.str.isupper()]\n",
    "#                  & (bigdf.W1.str.len() > 1) & (bigdf.W2.str.len() > 1)]\n",
    "\n",
    "def filter_phrases(bigrams: {' ': '_'}, phrases: {' ': '_'}):\n",
    "    phrases_ = set(phrases.values())\n",
    "    is_subset = lambda x: x not in phrases_ and not any(x in ent for ent in phrases_)\n",
    "    return z.valfilter(is_subset, bigrams)\n",
    "\n",
    "\n",
    "iscap = lambda x: x[0].isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigdf = get_bigrams(tokenize(all_text), δ=9)\n",
    "bigram_rep = dict(zip(bigdf.W1 + ' ' + bigdf.W2, bigdf.W1 + '_' + bigdf.W2))\n",
    "bigram_rep = filter_phrases(bigram_rep, phrase2wds)\n",
    "bigram_rep.update({\n",
    "        w: '_'.join(w.split()) for w in\n",
    "        'Mrs Norris;Every Flavor Beans;Expecto Patronum;Boy Who Lived'.split(';')})\n",
    "bigram_rep = z.keymap(lambda x: x.replace('Mrs ', 'Mrs. '), bigram_rep)\n",
    "punct_replace = {\"St Mungo's\": \"St. Mungo's\"}\n",
    "bigram_rep = z.keymap(lambda x: punct_replace.get(x, x), bigram_rep)\n",
    "bigram_rep = z.keyfilter(lambda x: x not in 'Every Flavor;Flavor Beans;Boy Who;Who Lived'.split(';'),\n",
    "                         bigram_rep )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrasify\n",
    "\n",
    "revdict = lambda d: {v: k for k, v in d.items()}\n",
    "rbigram_rep = revdict(bigram_rep)\n",
    "rphrase2wds = revdict(phrase2wds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('src/name2phrase.txt', 'r') as f:\n",
    "    name2phrase = dict(map(str.split, f.read().splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "to_replace = z.merge(phrase2wds, bigram_rep)\n",
    "multicase_phrases = ut.get_multi_case(OrderedDict(sorted(to_replace.items(), key=lambda x: -len(x[0]))), all_text)\n",
    "to_replace.update(multicase_phrases)\n",
    "phrased_text = multi_replace(to_replace, all_text)\n",
    "# phrased_text = multi_replace(phrase2wds.items(), all_text)\n",
    "toks = tokenize(phrased_text)\n",
    "toks = np.array([name2phrase.get(t, t) for t in toks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toksl = {t.lower() for t in toks}\n",
    "diffs = {t.lower() for t in z.valmap(lambda t: name2phrase.get(t, t), to_replace).values()} - toksl\n",
    "diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toks = tokenize(phrased_text)\n",
    "\n",
    "for t in toks:\n",
    "    if 'Vernon_Dursley' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "casedat = (DataFrame([(k, len(set(ut.find_all(all_text, k))),\n",
    "                       len(set(ut.find_all(lt, k.lower()))))\n",
    "                      for k, v in to_replace.items()], columns=['Word', 'Case', 'Nocase'])\n",
    "           .query('Case != Nocase').assign(Ratio = lambda x: x.eval('Case / Nocase'))\n",
    "           .sort_values('Ratio', ascending=True).reset_index(drop=1)\n",
    "          )\n",
    "\n",
    "    # casedat['Ratio'] = casedat.eval('Case / Nocase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "sp.sparse.csr_matrix.__matmul__ = sp.sparse.csr_matrix.dot\n",
    "# import sklearn\n",
    "\n",
    "# import autograd.numpy as np\n",
    "# from autograd import grad\n",
    "import numpy as np\n",
    "# %load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wordvec_utils as wut; reload(wut);\n",
    "import utils as ut; reload(ut);\n",
    "import test; reload(test);\n",
    "from wordvec_utils import Cat, WordVectorizer, get_rand_wins, get_wins\n",
    "from voluptuous import Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THRESH = 0.0014\n",
    "THRESH = 0.15\n",
    "\n",
    "def get_subsample_prob(txt, thresh=.001):\n",
    "    cts = Counter(txt)\n",
    "    freq = np.array([cts[w] for w in txt]) / sum(cts.values())\n",
    "    p = 1 - np.sqrt(thresh / freq)\n",
    "    return np.clip(p, 0, 1)\n",
    "\n",
    "\n",
    "def get_subsample(txt, thresh=.001):\n",
    "    \"\"\"\n",
    "    Drop words with frequency above given threshold according to frequency.\n",
    "    From \"Distributed Representations of Words and Phrases and their Compositionality\"\n",
    "    Returns pair of (left in words, left out words)\n",
    "    \"\"\"\n",
    "    p = get_subsample_prob(txt, thresh=thresh)\n",
    "    drop = np.zeros_like(p, dtype=bool)\n",
    "\n",
    "    for pval in sorted(set(p[p > 0]), reverse=1):\n",
    "        bm = p == pval\n",
    "        n = bm.sum()\n",
    "        pdrop = nr.random(n) < pval\n",
    "        drop[bm] = pdrop\n",
    "    \n",
    "    print('Dropping {:.2%} of words'.format(drop.mean()))\n",
    "    return txt[~drop], txt[drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "threshs = wut.inspect_freq_thresh(toks).sort_values('Freq').reset_index(drop=1)\n",
    "[ix] = threshs.Freq.sort_values().searchsorted(THRESH)\n",
    "threshs.iloc[ix-5:ix+5]\n",
    "\n",
    "threshs[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective functions\n",
    "\n",
    "The equation for the skip-gram objective function is the following\n",
    "\\begin{align}\n",
    "E & = -\\log \\prod_{c=1} ^{C}\n",
    "    \\frac {\\exp (u_{c,j^*_c})}\n",
    "          {\\sum_{j'=1} ^ V \\exp(u_{j'})} \\\\\n",
    "  & = -\\sum^C_{c=1} u_{j^*_c} + C \\cdot \\log \\sum ^ V _{j'=1} \\exp(u_j')\n",
    "\\end{align}\n",
    "and implemented below as `vanilla_likelihood`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vanilla_likelihood(wi, cwds, dv=None):\n",
    "    wix = dv.get(wi)\n",
    "    cixs = dv.get(cwds)\n",
    "    C = len(cwds)\n",
    "\n",
    "    def logloss(Wall):\n",
    "        W1, W2 = Cat.split(Wall)\n",
    "        h = W1[wix, :]  # ∈ ℝⁿ\n",
    "        u = np.dot(h, W2)  # u[1083] == 427  ∈ ℝⱽ\n",
    "        ucs = u[cixs]\n",
    "        return -np.sum(ucs) + C * np.log(np.sum(np.exp(u)))\n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading about the negative sampling modification, however, I replaced the usage of the above with `ns_obj`, written later.\n",
    "\n",
    "### Negative sampling\n",
    "#### Unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "# from autograd.numpy import exp, log\n",
    "from numpy import exp, log\n",
    "from builtins import zip as izip, range as xrange\n",
    "\n",
    "nopython = jit(nopython=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def bisect_left(a, v):\n",
    "    \"\"\"Based on bisect module at (commit 1fe0fd9f)\n",
    "    cpython/blob/master/Modules%2F_bisectmodule.c#L150 \n",
    "    \"\"\"\n",
    "    lo, hi = 0, len(a)\n",
    "    while (lo < hi):\n",
    "        mid = (lo + hi) // 2\n",
    "        if a[mid] < v:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            hi = mid\n",
    "    return lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigram(txt, pow=.75):\n",
    "    \"Unigram^(3/4) model\"\n",
    "    cts = Series(Counter(txt))\n",
    "    \n",
    "    # If txt is integers, fill in missing values (likely for unknown token)\n",
    "    # with 0 probability to reliably use index to identify token\n",
    "    int_txt = cts.index.dtype == int\n",
    "    if int_txt:\n",
    "        missing_tokens = set(range(cts.index.max())) - set(cts.index)\n",
    "        for msg in missing_tokens:\n",
    "            cts.loc[msg] = 0\n",
    "        cts = cts.sort_index()\n",
    "        \n",
    "    N = len(txt)\n",
    "    ctsdf = ((cts / cts.sum()) ** pow).reset_index(drop=0)\n",
    "    ctsdf.columns = ['Word', 'Prob']\n",
    "    if int_txt:\n",
    "        assert (ctsdf.Word == ctsdf.index).all()\n",
    "    return ctsdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_sampler_pd(xs, K, pow=.75):\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    for seed in count():\n",
    "        yield ug.Word.sample(n=K, weights=ug.Prob, random_state=seed, replace=True)\n",
    "        \n",
    "        \n",
    "def neg_sampler_np(xs, K, cache_len=1000, use_seed=False, pow=.75):\n",
    "    \"Faster neg. sampler without the pandas overhead\"\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    p = ug.Prob.values / ug.Prob.sum()\n",
    "    a = ug.Word.values\n",
    "\n",
    "    for seed in count():\n",
    "        if use_seed:\n",
    "            nr.seed(seed)\n",
    "        Wds = nr.choice(a, size=(cache_len, K), p=p)\n",
    "        for wds in Wds:\n",
    "            yield wds\n",
    "            \n",
    "            \n",
    "def neg_sampler_np_l(xs, K, cache_len=1000, pow=.75):\n",
    "    \"Faster neg. sampler without the pandas overhead\"\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    p = ug.Prob.values / ug.Prob.sum()\n",
    "    a = list(ug.Word.values)\n",
    "\n",
    "    @nopython\n",
    "    def sample_():\n",
    "        while 1:\n",
    "            Wds = nr.choice(a, size=(cache_len, K), p=p)\n",
    "            for i in xrange(len(Wds)):\n",
    "                yield Wds[i]\n",
    "    return sample_\n",
    "\n",
    "\n",
    "def neg_sampler_j(xs, K, pow=.75):\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    cum_prob = ug.Prob.cumsum() / ug.Prob.sum()\n",
    "    \n",
    "    @nopython\n",
    "    def sample_(cum_prob, K):\n",
    "        while 1:\n",
    "            l = []\n",
    "            for i in xrange(K):\n",
    "                l.append(bisect_left(cum_prob, nr.rand()))\n",
    "            yield l\n",
    "\n",
    "    return sample_(cum_prob.values, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "gen = sample_(ug.Cum_prob.values, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "toks = le.fit_transform(all_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genj = neg_sampler_j(toks, 8)\n",
    "gennp = neg_sampler_np(toks, 8)\n",
    "genp = neg_sampler_pd(toks, 8)\n",
    "\n",
    "next(genj); next(gennp); next(genp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "%time csj = Series(Counter(x for xs in it.islice(genj, n) for x in xs))\n",
    "%time csnp = Series(Counter(x for xs in it.islice(gennp, n) for x in xs))\n",
    "%time csp = Series(Counter(x for xs in it.islice(genp, n // 100) for x in xs))\n",
    "\n",
    "ug = unigram(toks, pow=.75)\n",
    "cts = DataFrame({'Numba': csj, 'Numpy': csnp, 'Pandas': csp}).fillna(0)\n",
    "probs = cts / cts.sum()\n",
    "probs['Probs'] = ug.Prob / ug.Prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_dist(xcol=None, subplt=None):\n",
    "    plt.subplot(subplt)\n",
    "    probs.plot(x=xcol, y='Probs', ax=plt.gca(), kind='scatter', alpha=.25)\n",
    "    _, xi = plt.xlim(None)\n",
    "    _, yi = plt.ylim(0, None)\n",
    "    end = min(xi, yi)\n",
    "    plt.plot([0, end], [0, end], alpha=.2)\n",
    "    \n",
    "plt.figure(figsize=(16, 10))\n",
    "plot_dist(xcol='Numba', subplt=131)\n",
    "plot_dist(xcol='Numpy', subplt=132)\n",
    "plot_dist(xcol='Pandas', subplt=133)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "list(sliding_window(ls[:10], C=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit list(sliding_window(ls, C=4))\n",
    "%timeit list(sliding_window2(ls, C=4))\n",
    "%timeit list(sliding_window3(ls, C=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E = -\\log \\sigma(\\boldsymbol v_{w_O}' ^T \\boldsymbol h)\n",
    "    - \\sum^K _{i=1} \\log \\sigma (-\\boldsymbol v_{w_i}' ^T \\boldsymbol h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial E}\n",
    "         {\\partial \\boldsymbol v_{w_j}' ^T \\boldsymbol h}\n",
    "         = \\sigma(\\boldsymbol v_{w_j}' ^T \\boldsymbol h) -t_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @nopython\n",
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# take = z.compose(list, it.islice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getNall = lambda W: W.shape[1] // 2\n",
    "\n",
    "@ut.memoize\n",
    "def mrange(*a):\n",
    "    return list(xrange(*a))\n",
    "\n",
    "def get_vecs1(Wall, w_ix: int=0, vo_ix: [int]=1, negsamp_ixs: [int]=None):\n",
    "    if negsamp_ixs is None:\n",
    "        negsamp_ixs = mrange(2, len(Wall))\n",
    "    N = getNall(Wall)\n",
    "    h = Wall[w_ix, :N]  # ∈ ℝⁿ\n",
    "    vwo = Wall[vo_ix, N:]\n",
    "    negsamps = Wall[negsamp_ixs, N:]\n",
    "    return h, vwo, negsamps\n",
    "\n",
    "\n",
    "def gen_labels(negsamps):\n",
    "    return [1] + [0] * len(negsamps)\n",
    "\n",
    "\n",
    "def ns_loss_grad_dot(h=None, vout=None, label=None):\n",
    "    return sig(vout @ h) - label\n",
    "\n",
    "\n",
    "def ns_loss_grads(h: 'v[n]', vout: '[v[n]]', label: 'v[n]'):\n",
    "    dot = ns_loss_grad_dot(h=h, vout=vout, label=label)\n",
    "    return dot * vout, dot * h\n",
    "\n",
    "\n",
    "def zeros(shape, z=ut.memoize(lambda shape: np.zeros(shape))):\n",
    "    return z(shape).copy()\n",
    "\n",
    "    \n",
    "def ns_grad(Wsub):\n",
    "    # global hgrad, vgrad, Wsub, N\n",
    "#     h, vwo, negsamps = get_vecs1jit(Wsub)\n",
    "    h, vwo, negsamps = get_vecs1(Wsub)\n",
    "    N = getNall(Wsub)\n",
    "    Wsub_grad = zeros(Wsub.shape)\n",
    "    \n",
    "    for i, vout, label in izip(count(1), it.chain([vwo], negsamps), gen_labels(negsamps)):\n",
    "        hgrad, vgrad = ns_loss_grads(h, vout, label)\n",
    "        Wsub_grad[0, :N] += hgrad\n",
    "        Wsub_grad[i, N:] += vgrad\n",
    "\n",
    "    return Wsub_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ns_grad(Wsub)\n",
    "%timeit ns_grad(Wsub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "The following gradient checking functionality based on [the UFLDL tutorial](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/) can be used to ensure that autograd is working as expected. It may be redundant with autograd calculating everything automatically, but I felt better checking manually for a few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def ns_loss(h, vwo, vwi_negs):\n",
    "    \"\"\"This should be called on the subset of the matrix (Win || Wout')\n",
    "    determined by row indices `wi, win_ix, negwds`.\n",
    "    Indexing relevant rows before passing to `logloss` seems to speed up autograd.\n",
    "    \"\"\"\n",
    "    # Win, Wout = Cat.split(Wall_sub)  # copy\n",
    "    # return -np.log(σ(np.dot(vwo.T, h))) - np.sum(np.log(σ(-np.dot(vwi_negs.T, h))))\n",
    "    # return -np.log(sig(vwo @ h)) - np.sum(np.log(sig(-vwi_negs @ h)))\n",
    "    negsum = 0\n",
    "    for j in xrange(len(vwi_negs)):\n",
    "        negsum += np.log(sig(-vwi_negs[j] @ h))\n",
    "        \n",
    "    return -np.log(sig(vwo @ h)) - negsum\n",
    "\n",
    "\n",
    "def ns_loss_vec(h, vwo, vwi_negs):\n",
    "    \"\"\"This should be called on the subset of the matrix (Win || Wout')\n",
    "    determined by row indices `wi, win_ix, negwds`.\n",
    "    Indexing relevant rows before passing to `logloss` seems to speed up autograd.\n",
    "    \"\"\"\n",
    "    # Win, Wout = Cat.split(Wall_sub)  # copy\n",
    "    # return -np.log(σ(np.dot(vwo.T, h))) - np.sum(np.log(σ(-np.dot(vwi_negs.T, h))))\n",
    "    # return -np.log(sig(vwo @ h)) - np.sum(np.log(sig(-vwi_negs @ h)))\n",
    "    return -np.log(sig(vwo @ h)) - np.sum(np.log(sig(-vnegs @ h )))\n",
    "\n",
    "# vnegs = Wout[:, neg_samps].T.copy()\n",
    "# ns_loss_jit = jit(nopython=1)(ns_loss)\n",
    "# ns_loss_vec_jit = jit(nopython=1)(ns_loss_vec)\n",
    "# a1 = ns_loss(h, vwo, vnegs)\n",
    "# a2 = ns_loss_jit(h, vwo, vnegs)\n",
    "# a3 = ns_loss_vec(h, vwo, vnegs)\n",
    "# a4 = ns_loss_vec_jit(h, vwo, vnegs)\n",
    "# assert np.isclose(a1, a2) and np.isclose(a1, a3) and np.isclose(a1, a4)\n",
    "# ns_loss = ns_loss_vec_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def J(Wsub):\n",
    "    N = getNall(Wsub)\n",
    "    h, vwo, vwi_negs = get_vecs1(Wsub)\n",
    "    # h, vwo, vwi_negs = Wsub[0, :N], Wsub[1, N:], Wsub[range(2, len(Wsub)), N:]\n",
    "    return ns_loss(h, vwo, vwi_negs)\n",
    "\n",
    "def check_grad_(W, i: int=None, j: int=None, eps=1e-6, J: Callable=None):\n",
    "    \"From eqn at http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/\"\n",
    "    Wneg, Wpos = W.copy(), W.copy()\n",
    "    Wneg[i, j] -= eps\n",
    "    Wpos[i, j] += eps\n",
    "    return (J(Wpos) - J(Wneg)) / (2 * eps)\n",
    "\n",
    "def approx_grad(W, J=J):\n",
    "    n, m = W.shape\n",
    "    grad = np.zeros_like(W)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            grad[i, j] = check_grad_(W, i=i, j=j, eps=1e-6, J=J)\n",
    "    return grad\n",
    "\n",
    "\n",
    "# DataFrame(approx_grad(Wsub))\n",
    "# J(Wsub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit ns_loss(h, vwo, vnegs)\n",
    "%timeit ns_loss_vec(h, vwo, vnegs)\n",
    "%timeit ns_loss_jit(h, vwo, vnegs)\n",
    "%timeit ns_loss_vec_jit(h, vwo, vnegs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import negsamp_grad; reload(negsamp_grad);\n",
    "from negsamp_grad import ns_grad as ns_grad_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = wut.init_w(1000, 50, seed=1)\n",
    "Wsub = W[:8]\n",
    "W.shape\n",
    "# ns_grad(Wsub)\n",
    "\n",
    "assert np.allclose(ns_grad(Wsub), ns_grad_jit(Wsub))\n",
    "assert np.allclose(approx_grad(Wsub), ns_grad_jit(Wsub))\n",
    "\n",
    "%timeit approx_grad(Wsub)\n",
    "%timeit ns_grad(Wsub)\n",
    "%timeit ns_grad_jit(Wsub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordvec_utils import Dict, Num, even, orig_type, update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils as ut; reload(ut);\n",
    "from voluptuous import ALLOW_EXTRA\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "def ping():\n",
    "    !say done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Conf = Schema(dict(\n",
    "        λ=Num,\n",
    "        norm=Num,  accumsec=Num, norms=Dict({int: float}),  gradnorms=Dict({int: float}),\n",
    "        N=int, K=int, term={}, iter=int, epoch=int, dir=str,\n",
    "        C=even,  # full window size; must be an even number\n",
    "        thresh=Num,  # gradient norm threshold for decreasing learning rate\n",
    "), extra=ALLOW_EXTRA, required=True)\n",
    "Conf = orig_type(Conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf = ut.AttrDict(\n",
    "    λ=.5, norm=0, accumsec=0, norms={}, gradnorms={}, N=100,\n",
    "    C=4, K=6, iter=0, thresh=15, epoch=0,\n",
    "    term=dict(iters=None,\n",
    "              secs=10\n",
    "    ),\n",
    "    dir='cache',\n",
    ")\n",
    "cnf = Conf(cnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(xs, C=4, start_pos=0):\n",
    "    \"\"\"Iterates through corpus, yielding input word\n",
    "    and surrounding context words\"\"\"\n",
    "    winsize = C // 2\n",
    "    N = len(xs)\n",
    "    for i, x in enumerate(xs, start_pos):\n",
    "        ix1 = max(0, i-winsize)\n",
    "        ix2 = min(N, i+winsize+1)\n",
    "        yield x, xs[ix1:i] + xs[i + 1:ix2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         if i % 1000 == 0:\n",
    "#             _gn = gradnorms[i + gradnormax] = np.linalg.norm(grad)\n",
    "#             maxnorms.append(_gn)\n",
    "#             norms[i + normax] = np.linalg.norm(W)\n",
    "#             if max(maxnorms) > cf.thresh:\n",
    "#                 cf['λ'] *= 2 / 3\n",
    "#                 print('Setting λ: {:.2f}'.format(cf['λ']))\n",
    "#                 maxnorms.clear()\n",
    "# #             else:\n",
    "# #                 print('{:.1f}'.format(max(maxnorms)), end='; ')\n",
    "#         # sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def grad_norm(Wsub):\n",
    "    \"\"\"Calculate norm of gradient, where first row\n",
    "    is input vector, rest are output vectors. For any row,\n",
    "    half of the entries are zeros, which allows a lot of\n",
    "    skipping for a faster computation\"\"\"\n",
    "    n = Wsub.shape[1] // 2\n",
    "    sm = 0\n",
    "    for i in xrange(n):\n",
    "        sm += Wsub[0, i] ** 2\n",
    "    for i in xrange(1, len(Wsub)):\n",
    "        for j in xrange(n, 2 * n):\n",
    "            sm += Wsub[i, j] ** 2\n",
    "    return np.sqrt(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grd = ns_grad_jit(Wsub)\n",
    "assert np.isclose(grad_norm(grd), np.linalg.norm(grad_norm(grd)))\n",
    "\n",
    "%timeit np.linalg.norm(grad_norm(grd))\n",
    "%timeit grad_norm(grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(W=None, corp=None, cf={}, ns_grad=ns_grad):\n",
    "    # TODO: ensure neg samp != wi\n",
    "    if not os.path.exists(cf.dir):\n",
    "        os.mkdir(cf.dir)\n",
    "    st = time.time(); cf = Conf(cf)  #.copy()\n",
    "    norms = dict(cf.norms); gradnorms = dict(cf.gradnorms)\n",
    "    assert cf.N == W.shape[1] / 2, 'shape of W disagrees with conf'\n",
    "    maxnorms = deque([], 5)\n",
    "\n",
    "    normax = max(norms or [0]); gradnormax = max(gradnorms or [0]);\n",
    "    # global Win, Wout, w, cont, negsamp_lst, c, negsamps, sub_ixs\n",
    "    # Win, Wout = Cat.split(W)\n",
    "    iter_corpus = corp[cf.iter:]\n",
    "    learning_rates = np.linspace(cf['λ'], cf['λ'] * .1, len(iter_corpus))\n",
    "    iters_ = izip(count(cf.iter),\n",
    "                  sliding_window(iter_corpus, C=cf.C),\n",
    "                  z.partition(cf.C, neg_sampler_np(corp, cf.K)),\n",
    "                  learning_rates,\n",
    "                 )\n",
    "    iters = ut.timeloop(iters_, **cf.term)\n",
    "\n",
    "    for i, (w, cont_), negsamp_lst, eta in iters:\n",
    "        cont = [x for x in cont_ if x != w] if w in cont_ else cont_\n",
    "        for c, negsamps_ in zip(cont, negsamp_lst):\n",
    "            negsamps = ([x for x in negsamps_ if x not in {w, c}]\n",
    "                        if set([w, c]) & set(negsamps_) else list(negsamps_))\n",
    "            sub_ixs = [w, c] + negsamps # list(negsamps)\n",
    "            Wsub = W[sub_ixs]\n",
    "            grad = ns_grad(Wsub)\n",
    "            gnorm = grad_norm(grad)\n",
    "            \n",
    "            if gnorm > 5:  # clip gradient\n",
    "                grad /= gnorm\n",
    "            W[sub_ixs] -= eta * grad    \n",
    "                \n",
    "        if i % 1000 == 0 and np.isnan(grad).any():\n",
    "            global grd\n",
    "            print('ruh roh!'); grd = grad\n",
    "            return\n",
    "\n",
    "    tdur = time.time() - st\n",
    "    print('{:.2f} mins'.format(tdur / 60))\n",
    "    cf2 = update(cf, norms=norms, gradnorms=gradnorms, iter=i+1)\n",
    "    cf2['accumsec'] += tdur\n",
    "    if not cf2.term:\n",
    "        DataFrame(W, index=vc).to_csv(os.path.join(cf2.dir, 'n{}_e{}.csv'.format(cf.N, cf.epoch)))\n",
    "        cf2['epoch'] += 1\n",
    "        cf2 = update(cf2, iter=0)\n",
    "    else:\n",
    "        print(i, 'iters')\n",
    "    # ping()\n",
    "    return W, cf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -T lp.txt -s -f sgd sgd(W=We.copy(), corp=toki, cf=update(cnfe, term={'iters': 10000}), ns_grad=ns_grad) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -T lp3.txt -s -f sgd sgd(W=We.copy(), corp=toki, cf=update(cnfe, term={'iters': 10000}), ns_grad=ns_grad_jit) # ls[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rand_ixs = lambda W, n=8, axis=0: nr.randint(0, W.shape[axis], size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def ineg(W, sub_ixs, grad):\n",
    "    # for i in xrange(len(grad)):\n",
    "    for i, _ in enumerate(grad):\n",
    "        W[sub_ixs[i]] -= grad[i]\n",
    "        \n",
    "def inegp(W, sub_ixs, grad):\n",
    "    W[sub_ixs] -= grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_ineg1(wsub_):\n",
    "    wsub = wsub_.copy()\n",
    "    wsub -= gr * .1\n",
    "    return wsub\n",
    "\n",
    "def test_ineg2(wsub_):\n",
    "    wsub = wsub_.copy()\n",
    "    ineg(wsub, gr)\n",
    "    wsub -= gr * .1\n",
    "    return wsub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ixs = [ 1, 4, 7, 99, 486, 263, 924]\n",
    "w1, w2 = wtst.copy(), wtst.copy()\n",
    "# w1, w2 = wtst[ixs].copy(), wtst[ixs].copy()\n",
    "gr = ns_grad(w1[ixs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit ineg(w1, ixs, gr)\n",
    "%timeit inegp(w2, ixs, gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.allclose(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ineg(w1, ixs, gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inegp(w2, ixs, gr)\n",
    "np.allclose(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtst[ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtst = wut.init_w(1000, 50)\n",
    "wsub = wtst[:8].copy()\n",
    "gr = ns_grad(wsub)\n",
    "# gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ineg1(wsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wsub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weight_normj(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_norm(grad), weight_norm2(grad), weight_normj(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_norm_jit(grad), np.linalg.norm(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit weight_norm(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit weight_norm2(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit weight_norm_jit(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit weight_normj(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.linalg.norm(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad, grad2 = grd, grd2\n",
    "Wsub = W2[[w, c] + list(negsamps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(approx_grad(Wsub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(Wsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(ns_grad(W2[[w, c] + list(negsamps)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print('Epoch {}'.format(cnfe.epoch))\n",
    "    We2, cnfe = sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(vc.__getitem__, [c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(vc.__getitem__, negsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negsamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del w, cont, c, negsamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(grad, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc[19276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toki[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgdpart = partial(sgd, W=W.copy(), corp=toki, cf=update(cnfe, term={'iters': 1}))\n",
    "_ = sgdpart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradnorm_vec = np.linalg.norm(grad, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if np.linalg.norm(gradnorm_vec) > 5:\n",
    "    grad = grad / gradnorm_vec[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradnormed = np.divide(grad, gradnorm)\n",
    "gradnormed = grad / gradnorm[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(gradnormed, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradnormed.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "7.12\n",
    "  1085718\n",
    "6.00742\n",
    "-> 684739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd(W=W.copy(), corp=toki, cf=update(cnf, term={'iters': 1}));\n",
    "%prun -qD prof.txt sgd(W=W.copy(), corp=toki, cf=update(cnf, term={'iters': 5000})) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnfe = update(cnf, C=4, iter=0, term=dict(), N=100, λ=.5, dir='cache/v12', epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tks = np.array(all_text.split())\n",
    "stoks, dropped = get_subsample(toks, thresh=THRESH)\n",
    "# assert 'Albus_Dumbledore' in stoks\n",
    "dv = WordVectorizer().fit(stoks)\n",
    "toki = [dv.vocabulary_[x] for x in stoks]\n",
    "vc = dv.feature_names_\n",
    "W = wut.init_w(len(vc), cnfe.N, seed=1)\n",
    "with open('txt.txt','w') as f:\n",
    "    f.write('\\n'.join(stoks))\n",
    "    \n",
    "We = W.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!say done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We = pd.read_csv('cache/v9/n15_e26.csv', index_col=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(*zip(*gns), alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print('Epoch {}'.format(cnfe.epoch))\n",
    "    We2, cnfe = sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "We2 - We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "W2, cnf2 = sgd(W=W.copy(), corp=toki, cf=update(cnf, term={'mins': 60})) # ls[:20]\n",
    "sw = sliding_window(toki, C=cnf.C)\n",
    "# 799509 iters\n",
    "# CPU times: user 12min 29s, sys: 4.47 s, total: 12min 34s\n",
    "# Wall time: 12min 35s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "W3, cnf3 = sgd(W=W2.copy(), corp=toki, cf=update(cnf2, iter=0)) # ls[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "W3, cnf2 = sgd(W=W2.copy(), corp=toki, cf=update(cnf, term={'mins': 5})) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W3, cnf3 = sgd(W=W3.copy(), corp=toki, cf=update(cnf3, term={'mins': 5}), ) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wd = DataFrame(W2, index=vc)\n",
    "wd.to_csv('cache/v2/n6_e1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd.mean(axis=1).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wd[10:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Series(cnfe.norms).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, x in enumerate([-1, 1,6,8]):\n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf2.norms[48000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "gnrms = Series(cnfe.gradnorms)\n",
    "gnrms.plot()\n",
    "pd.rolling_mean(gnrms, 20).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update equation\n",
    "$$\n",
    "    \\frac{\\partial E}\n",
    "         {\\partial \\boldsymbol v_{w_j}' ^T \\boldsymbol h}\n",
    "         = \\sigma(\\boldsymbol v_{w_j}' ^T \\boldsymbol h) -t_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyze word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
