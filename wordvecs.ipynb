{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var csc = IPython.keyboard_manager.command_shortcuts\n",
    "csc.add_shortcut('Ctrl-k','ipython.move-selected-cell-up')\n",
    "csc.add_shortcut('Ctrl-j','ipython.move-selected-cell-down')\n",
    "csc.add_shortcut('Shift-m','ipython.merge-selected-cell-with-cell-after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from project_imports import *\n",
    "import utils as ut; reload(ut);\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import scipy as sp\n",
    "# sp.sparse.csr_matrix.__matmul__ = sp.sparse.csr_matrix.dot\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wordvec_utils as wut; reload(wut);\n",
    "from wordvec_utils import Cat, WordVectorizer\n",
    "from voluptuous import Schema, ALLOW_EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESH = 0.15\n",
    "\n",
    "def get_subsample_prob(txt, thresh=.001):\n",
    "    cts = Counter(txt)\n",
    "    freq = np.array([cts[w] for w in txt]) / sum(cts.values())\n",
    "    p = 1 - np.sqrt(thresh / freq)\n",
    "    return np.clip(p, 0, 1)\n",
    "\n",
    "\n",
    "def get_subsample(txt, thresh=.001) -> (['keep'], ['drop']):\n",
    "    \"\"\"\n",
    "    Drop words with frequency above given threshold according to frequency.\n",
    "    From \"Distributed Representations of Words and Phrases and their Compositionality\"\n",
    "    Returns pair of (left in words, left out words)\n",
    "    \"\"\"\n",
    "    p = get_subsample_prob(txt, thresh=thresh)\n",
    "    drop = np.zeros_like(p, dtype=bool)\n",
    "\n",
    "    for pval in sorted(set(p[p > 0]), reverse=1):\n",
    "        bm = p == pval\n",
    "        n = bm.sum()\n",
    "        pdrop = nr.random(n) < pval\n",
    "        drop[bm] = pdrop\n",
    "    \n",
    "    print('Dropping {:.2%} of words'.format(drop.mean()))\n",
    "    return txt[~drop], txt[drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "threshs = wut.inspect_freq_thresh(toks).sort_values('Freq').reset_index(drop=1)\n",
    "[ix] = threshs.Freq.sort_values().searchsorted(THRESH)\n",
    "threshs.iloc[ix-5:ix+5]\n",
    "\n",
    "threshs[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective functions\n",
    "\n",
    "The equation for the skip-gram objective function is the following\n",
    "\\begin{align}\n",
    "E & = -\\log \\prod_{c=1} ^{C}\n",
    "    \\frac {\\exp (u_{c,j^*_c})}\n",
    "          {\\sum_{j'=1} ^ V \\exp(u_{j'})} \\\\\n",
    "  & = -\\sum^C_{c=1} u_{j^*_c} + C \\cdot \\log \\sum ^ V _{j'=1} \\exp(u_j')\n",
    "\\end{align}\n",
    "and implemented below as `vanilla_likelihood`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vanilla_likelihood(wi, cwds, dv=None):\n",
    "    wix = dv.get(wi)\n",
    "    cixs = dv.get(cwds)\n",
    "    C = len(cwds)\n",
    "\n",
    "    def logloss(Wall):\n",
    "        W1, W2 = Cat.split(Wall)\n",
    "        h = W1[wix, :]  # ∈ ℝⁿ\n",
    "        u = np.dot(h, W2)  # u[1083] == 427  ∈ ℝⱽ\n",
    "        ucs = u[cixs]\n",
    "        return -np.sum(ucs) + C * np.log(np.sum(np.exp(u)))\n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading about the negative sampling modification, however, I replaced the usage of the above with `ns_obj`, written later.\n",
    "\n",
    "### Negative sampling\n",
    "#### Unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "# from autograd.numpy import exp, log\n",
    "from numpy import exp, log\n",
    "from builtins import zip as izip, range as xrange\n",
    "\n",
    "nopython = jit(nopython=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def bisect_left(a, v):\n",
    "    \"\"\"Based on bisect module at (commit 1fe0fd9f)\n",
    "    cpython/blob/master/Modules%2F_bisectmodule.c#L150 \n",
    "    \"\"\"\n",
    "    lo, hi = 0, len(a)\n",
    "    while (lo < hi):\n",
    "        mid = (lo + hi) // 2\n",
    "        if a[mid] < v:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            hi = mid\n",
    "    return lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigram(txt, pow=.75):\n",
    "    \"Unigram^(3/4) model\"\n",
    "    cts = Series(Counter(txt))\n",
    "    \n",
    "    # If txt is integers, fill in missing values (likely for unknown token)\n",
    "    # with 0 probability to reliably use index to identify token\n",
    "    int_txt = cts.index.dtype == int\n",
    "    if int_txt:\n",
    "        missing_tokens = set(range(cts.index.max())) - set(cts.index)\n",
    "        for msg in missing_tokens:\n",
    "            cts.loc[msg] = 0\n",
    "        cts = cts.sort_index()\n",
    "        \n",
    "    N = len(txt)\n",
    "    ctsdf = ((cts / cts.sum()) ** pow).reset_index(drop=0)\n",
    "    ctsdf.columns = ['Word', 'Prob']\n",
    "    if int_txt:\n",
    "        assert (ctsdf.Word == ctsdf.index).all()\n",
    "    return ctsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_sampler_pd(xs, K, pow=.75):\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    for seed in count():\n",
    "        yield ug.Word.sample(n=K, weights=ug.Prob, random_state=seed, replace=True)\n",
    "        \n",
    "        \n",
    "def neg_sampler_np(xs, K, cache_len=1000, use_seed=False, pow=.75):\n",
    "    \"Faster neg. sampler without the pandas overhead\"\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    p = ug.Prob.values / ug.Prob.sum()\n",
    "    a = ug.Word.values\n",
    "\n",
    "    for seed in count():\n",
    "        if use_seed:\n",
    "            nr.seed(seed)\n",
    "        Wds = nr.choice(a, size=(cache_len, K), p=p)\n",
    "        for wds in Wds:\n",
    "            yield wds\n",
    "            \n",
    "            \n",
    "def neg_sampler_np_l(xs, K, cache_len=1000, pow=.75):\n",
    "    \"Faster neg. sampler without the pandas overhead\"\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    p = ug.Prob.values / ug.Prob.sum()\n",
    "    a = list(ug.Word.values)\n",
    "\n",
    "    @nopython\n",
    "    def sample_():\n",
    "        while 1:\n",
    "            Wds = nr.choice(a, size=(cache_len, K), p=p)\n",
    "            for i in xrange(len(Wds)):\n",
    "                yield Wds[i]\n",
    "    return sample_\n",
    "\n",
    "\n",
    "def neg_sampler_j(xs, K, pow=.75):\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    cum_prob = ug.Prob.cumsum() / ug.Prob.sum()\n",
    "    return neg_sampler_j_(cum_prob.values, K)\n",
    "\n",
    "@nopython\n",
    "def neg_sampler_j_(cum_prob, K):\n",
    "    while 1:\n",
    "        l = []\n",
    "        for i in xrange(K):\n",
    "            l.append(bisect_left(cum_prob, nr.rand()))\n",
    "        yield l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "gen = sample_(ug.Cum_prob.values, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "toks = le.fit_transform(all_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit genj = neg_sampler_j(toks, 8)\n",
    "%timeit gennp = neg_sampler_np(toks, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gennp = neg_sampler_np(toks, 8)\n",
    "%lprun -s -f neg_sampler_np next(gennp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -s -f unigram neg_sampler_j(toks, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genj = neg_sampler_j(toks, 8)\n",
    "gennp = neg_sampler_np(toks, 8)\n",
    "genp = neg_sampler_pd(toks, 8)\n",
    "\n",
    "next(genj); next(gennp); next(genp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "%time csp = Series(Counter(x for xs in it.islice(genp, n // 100) for x in xs))\n",
    "%time csnp = Series(Counter(x for xs in it.islice(gennp, n) for x in xs))\n",
    "%time csj = Series(Counter(x for xs in it.islice(genj, n) for x in xs))\n",
    "\n",
    "ug = unigram(toks, pow=.75)\n",
    "cts = DataFrame({'Numba': csj, 'Numpy': csnp, 'Pandas': csp}).fillna(0)\n",
    "probs = cts / cts.sum()\n",
    "probs['Probs'] = ug.Prob / ug.Prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_dist(xcol=None, subplt=None):\n",
    "    plt.subplot(subplt)\n",
    "    probs.plot(x=xcol, y='Probs', ax=plt.gca(), kind='scatter', alpha=.25)\n",
    "    _, xi = plt.xlim(None)\n",
    "    _, yi = plt.ylim(0, None)\n",
    "    end = min(xi, yi)\n",
    "    plt.plot([0, end], [0, end], alpha=.2)\n",
    "    \n",
    "plt.figure(figsize=(16, 10))\n",
    "plot_dist(xcol='Numba', subplt=131)\n",
    "plot_dist(xcol='Numpy', subplt=132)\n",
    "plot_dist(xcol='Pandas', subplt=133)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "list(sliding_window(ls[:10], C=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit list(sliding_window(ls, C=4))\n",
    "%timeit list(sliding_window2(ls, C=4))\n",
    "%timeit list(sliding_window3(ls, C=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E = -\\log \\sigma(\\boldsymbol v_{w_O}' ^T \\boldsymbol h)\n",
    "    - \\sum^K _{i=1} \\log \\sigma (-\\boldsymbol v_{w_i}' ^T \\boldsymbol h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial E}\n",
    "         {\\partial \\boldsymbol v_{w_j}' ^T \\boldsymbol h}\n",
    "         = \\sigma(\\boldsymbol v_{w_j}' ^T \\boldsymbol h) -t_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @nopython\n",
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# take = z.compose(list, it.islice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getNall = lambda W: W.shape[1] // 2\n",
    "\n",
    "@ut.memoize\n",
    "def mrange(*a):\n",
    "    return list(xrange(*a))\n",
    "\n",
    "def get_vecs1(Wall, w_ix: int=0, vo_ix: [int]=1, negsamp_ixs: [int]=None):\n",
    "    if negsamp_ixs is None:\n",
    "        negsamp_ixs = mrange(2, len(Wall))\n",
    "    N = getNall(Wall)\n",
    "    h = Wall[w_ix, :N]  # ∈ ℝⁿ\n",
    "    vwo = Wall[vo_ix, N:]\n",
    "    negsamps = Wall[negsamp_ixs, N:]\n",
    "    return h, vwo, negsamps\n",
    "\n",
    "\n",
    "def gen_labels(negsamps):\n",
    "    return [1] + [0] * len(negsamps)\n",
    "\n",
    "\n",
    "def ns_loss_grad_dot(h=None, vout=None, label=None):\n",
    "    return sig(vout @ h) - label\n",
    "\n",
    "\n",
    "def ns_loss_grads(h: 'v[n]', vout: '[v[n]]', label: 'v[n]'):\n",
    "    dot = ns_loss_grad_dot(h=h, vout=vout, label=label)\n",
    "    return dot * vout, dot * h\n",
    "\n",
    "\n",
    "def zeros(shape, z=ut.memoize(lambda shape: np.zeros(shape))):\n",
    "    return z(shape).copy()\n",
    "\n",
    "    \n",
    "def ns_grad(Wsub):\n",
    "    # global hgrad, vgrad, Wsub, N\n",
    "#     h, vwo, negsamps = get_vecs1jit(Wsub)\n",
    "    h, vwo, negsamps = get_vecs1(Wsub)\n",
    "    N = getNall(Wsub)\n",
    "    Wsub_grad = zeros(Wsub.shape)\n",
    "    \n",
    "    for i, vout, label in izip(count(1), it.chain([vwo], negsamps), gen_labels(negsamps)):\n",
    "        hgrad, vgrad = ns_loss_grads(h, vout, label)\n",
    "        Wsub_grad[0, :N] += hgrad\n",
    "        Wsub_grad[i, N:] += vgrad\n",
    "\n",
    "    return Wsub_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ns_grad(Wsub)\n",
    "%timeit ns_grad(Wsub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "The following gradient checking functionality based on [the UFLDL tutorial](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/) can be used to ensure that autograd is working as expected. It may be redundant with autograd calculating everything automatically, but I felt better checking manually for a few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def ns_loss(h, vwo, vwi_negs):\n",
    "    \"\"\"This should be called on the subset of the matrix (Win || Wout')\n",
    "    determined by row indices `wi, win_ix, negwds`.\n",
    "    Indexing relevant rows before passing to `logloss` seems to speed up autograd.\n",
    "    \"\"\"\n",
    "    # Win, Wout = Cat.split(Wall_sub)  # copy\n",
    "    # return -np.log(σ(np.dot(vwo.T, h))) - np.sum(np.log(σ(-np.dot(vwi_negs.T, h))))\n",
    "    # return -np.log(sig(vwo @ h)) - np.sum(np.log(sig(-vwi_negs @ h)))\n",
    "    negsum = 0\n",
    "    for j in xrange(len(vwi_negs)):\n",
    "        negsum += np.log(sig(-vwi_negs[j] @ h))\n",
    "        \n",
    "    return -np.log(sig(vwo @ h)) - negsum\n",
    "\n",
    "\n",
    "def ns_loss_vec(h, vwo, vwi_negs):\n",
    "    \"\"\"This should be called on the subset of the matrix (Win || Wout')\n",
    "    determined by row indices `wi, win_ix, negwds`.\n",
    "    Indexing relevant rows before passing to `logloss` seems to speed up autograd.\n",
    "    \"\"\"\n",
    "    # Win, Wout = Cat.split(Wall_sub)  # copy\n",
    "    # return -np.log(σ(np.dot(vwo.T, h))) - np.sum(np.log(σ(-np.dot(vwi_negs.T, h))))\n",
    "    # return -np.log(sig(vwo @ h)) - np.sum(np.log(sig(-vwi_negs @ h)))\n",
    "    return -np.log(sig(vwo @ h)) - np.sum(np.log(sig(-vnegs @ h )))\n",
    "\n",
    "# vnegs = Wout[:, neg_samps].T.copy()\n",
    "# ns_loss_jit = jit(nopython=1)(ns_loss)\n",
    "# ns_loss_vec_jit = jit(nopython=1)(ns_loss_vec)\n",
    "# a1 = ns_loss(h, vwo, vnegs)\n",
    "# a2 = ns_loss_jit(h, vwo, vnegs)\n",
    "# a3 = ns_loss_vec(h, vwo, vnegs)\n",
    "# a4 = ns_loss_vec_jit(h, vwo, vnegs)\n",
    "# assert np.isclose(a1, a2) and np.isclose(a1, a3) and np.isclose(a1, a4)\n",
    "# ns_loss = ns_loss_vec_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def J(Wsub):\n",
    "    N = getNall(Wsub)\n",
    "    h, vwo, vwi_negs = get_vecs1(Wsub)\n",
    "    # h, vwo, vwi_negs = Wsub[0, :N], Wsub[1, N:], Wsub[range(2, len(Wsub)), N:]\n",
    "    return ns_loss(h, vwo, vwi_negs)\n",
    "\n",
    "def check_grad_(W, i: int=None, j: int=None, eps=1e-6, J: Callable=None):\n",
    "    \"From eqn at http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/\"\n",
    "    Wneg, Wpos = W.copy(), W.copy()\n",
    "    Wneg[i, j] -= eps\n",
    "    Wpos[i, j] += eps\n",
    "    return (J(Wpos) - J(Wneg)) / (2 * eps)\n",
    "\n",
    "def approx_grad(W, J=J):\n",
    "    n, m = W.shape\n",
    "    grad = np.zeros_like(W)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            grad[i, j] = check_grad_(W, i=i, j=j, eps=1e-6, J=J)\n",
    "    return grad\n",
    "\n",
    "\n",
    "# DataFrame(approx_grad(Wsub))\n",
    "# J(Wsub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit ns_loss(h, vwo, vnegs)\n",
    "%timeit ns_loss_vec(h, vwo, vnegs)\n",
    "%timeit ns_loss_jit(h, vwo, vnegs)\n",
    "%timeit ns_loss_vec_jit(h, vwo, vnegs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import negsamp_grad; reload(negsamp_grad);\n",
    "from negsamp_grad import ns_grad as ns_grad_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = wut.init_w(1000, 50, seed=1)\n",
    "Wsub = W[:8]\n",
    "W.shape\n",
    "# ns_grad(Wsub)\n",
    "\n",
    "assert np.allclose(ns_grad(Wsub), ns_grad_jit(Wsub))\n",
    "assert np.allclose(approx_grad(Wsub), ns_grad_jit(Wsub))\n",
    "\n",
    "%timeit approx_grad(Wsub)\n",
    "%timeit ns_grad(Wsub)\n",
    "%timeit ns_grad_jit(Wsub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordvec_utils import Dict, Num, even, orig_type, update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils as ut; reload(ut);\n",
    "from voluptuous import ALLOW_EXTRA\n",
    "from collections import deque\n",
    "\n",
    "def ping():\n",
    "    !say done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Conf = Schema(dict(\n",
    "        eta=Num, min_eta=Num,\n",
    "        norm=Num,  accumsec=Num, norms=Dict({int: float}),  gradnorms=Dict({int: float}),\n",
    "        N=int, K=int, term={}, iter=int, epoch=int, dir=str,\n",
    "        C=even,  # full window size; must be an even number\n",
    "        thresh=Num,  # gradient norm threshold for decreasing learning rate\n",
    "), extra=ALLOW_EXTRA, required=True)\n",
    "Conf = orig_type(Conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf = ut.AttrDict(\n",
    "    eta=.1, min_eta=.0001, norm=0, accumsec=0, norms={}, gradnorms={}, N=100,\n",
    "    C=4, K=6, iter=0, thresh=15, epoch=0,\n",
    "    term=dict(iters=None,\n",
    "              secs=10\n",
    "    ),\n",
    "    dir='cache',\n",
    ")\n",
    "cnf = Conf(cnf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sliding_window(xs, C=4, start_pos=0):\n",
    "    \"\"\"Iterates through corpus, yielding input word\n",
    "    and surrounding context words\"\"\"\n",
    "    #assert isinstance(xs, list)\n",
    "    winsize = C // 2\n",
    "    N = len(xs)\n",
    "    for i, x in enumerate(xs, start_pos):\n",
    "        ix1 = max(0, i-winsize)\n",
    "        ix2 = min(N, i+winsize+1)\n",
    "        yield x, xs[ix1:i] + xs[i + 1:ix2]\n",
    "\n",
    "\n",
    "@nopython\n",
    "def bounds_check_window(i, xs: [int], winsize, N):\n",
    "    x = xs[i]\n",
    "    ix1 = max(0, i-winsize)\n",
    "    ix2 = min(N, i+winsize+1)\n",
    "    return x, xs[ix1:i] + xs[i + 1:ix2]\n",
    "\n",
    "\n",
    "@nopython\n",
    "def sliding_window_jit(xs, C=4):\n",
    "    \"\"\"Iterates through corpus, yielding input word\n",
    "    and surrounding context words\"\"\"\n",
    "    winsize = C // 2\n",
    "    N = len(xs)\n",
    "    for i in xrange(winsize):\n",
    "        yield bounds_check_window(i, xs, winsize, N)\n",
    "    for i in xrange(winsize, N-winsize):\n",
    "        context = []\n",
    "        for j in xrange(i-winsize, i+winsize+1):\n",
    "            if j != i:\n",
    "                context.append(xs[j])\n",
    "        yield xs[i], context  # xs[i-winsize:i] + xs[i + 1:i+winsize+1]\n",
    "    for i in xrange(N-winsize, N):\n",
    "        yield bounds_check_window(i, xs, winsize, N)\n",
    "      \n",
    "@nopython\n",
    "def concat(a, b):\n",
    "    na = len(a)\n",
    "    n = na + len(b)\n",
    "    c = np.empty(n, dtype=a.dtype)\n",
    "    for i in xrange(na):\n",
    "        c[i] = a[i]\n",
    "    for i in xrange(len(b)):\n",
    "        c[i + na] = b[i]\n",
    "    return c\n",
    "\n",
    "@nopython\n",
    "def bounds_check_window_arr(i, xs: np.array, winsize, N):\n",
    "    x = xs[i]\n",
    "    ix1 = max(0, i-winsize)\n",
    "    ix2 = min(N, i+winsize+1)\n",
    "    return x, concat(xs[ix1:i], xs[i + 1:ix2])\n",
    "\n",
    "@nopython\n",
    "def sliding_window_jit_arr(xs, C=4):\n",
    "    \"\"\"Iterates through corpus, yielding input word\n",
    "    and surrounding context words\"\"\"\n",
    "    winsize = C // 2\n",
    "    N = len(xs)\n",
    "    for i in xrange(winsize):\n",
    "        yield bounds_check_window_arr(i, xs, winsize, N)\n",
    "    for i in xrange(winsize, N-winsize):\n",
    "        context = np.empty(C, dtype=np.int64)\n",
    "        for ci in xrange(winsize):\n",
    "            context[ci] = xs[i - winsize + ci]\n",
    "            context[winsize + ci] = xs[i + 1 + ci]\n",
    "#             if j != i:\n",
    "#                 context[ci] = xs[i - winsize + ci]\n",
    "        yield xs[i], context  # xs[i-winsize:i] + xs[i + 1:i+winsize+1]\n",
    "    for i in xrange(N-winsize, N):\n",
    "        yield bounds_check_window_arr(i, xs, winsize, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bounds_check_window(1, toks, 4, len(toks))\n",
    "bounds_check_window_arr(0, toks, 4, len(toks))\n",
    "bounds_check_window(i, xs, winsize, N)\n",
    "\n",
    "s = sliding_window_jit_arr(samp_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp_toks = nr.randint(0, 1e6, size=100005)\n",
    "samp_toksl = list(samp_toks)\n",
    "list(sliding_window_jit(samp_toksl[:100]))\n",
    "run_window = lambda f, toks=samp_toksl: list(f(toks))\n",
    "\n",
    "%timeit run_window(sliding_window)\n",
    "%timeit run_window(sliding_window_jit)\n",
    "%timeit run_window(sliding_window_jit_arr, toks=samp_toks)\n",
    "# assert run_window(sliding_window) == run_window(sliding_window_jit) == run_window(sliding_window_jit_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def grad_norm(Wsub):\n",
    "    \"\"\"Calculate norm of gradient, where first row\n",
    "    is input vector, rest are output vectors. For any row,\n",
    "    half of the entries are zeros, which allows a lot of\n",
    "    skipping for a faster computation\"\"\"\n",
    "    n = Wsub.shape[1] // 2\n",
    "    sm = 0\n",
    "    for i in xrange(n):\n",
    "        sm += Wsub[0, i] ** 2\n",
    "    for i in xrange(1, len(Wsub)):\n",
    "        for j in xrange(n, 2 * n):\n",
    "            sm += Wsub[i, j] ** 2\n",
    "    return np.sqrt(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grd = ns_grad_jit(Wsub)\n",
    "assert np.isclose(grad_norm(grd), np.linalg.norm(grad_norm(grd)))\n",
    "\n",
    "%timeit np.linalg.norm(grad_norm(grd))\n",
    "%timeit grad_norm(grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(W=None, corp=None, cf={}, ns_grad=ns_grad, neg_sampler=None, vc=None, sliding_window=sliding_window):\n",
    "    # TODO: ensure neg samp != wi\n",
    "    if not os.path.exists(cf.dir):\n",
    "        os.mkdir(cf.dir)\n",
    "    st = time.time(); cf = Conf(cf)  #.copy()\n",
    "    norms = dict(cf.norms); gradnorms = dict(cf.gradnorms)\n",
    "    assert cf.N == W.shape[1] / 2, 'shape of W disagrees with conf'\n",
    "    maxnorms = deque([], 5)\n",
    "\n",
    "    normax = max(norms or [0]); gradnormax = max(gradnorms or [0]);\n",
    "    # global Win, Wout, w, cont, negsamp_lst, c, negsamps, sub_ixs\n",
    "    # Win, Wout = Cat.split(W)\n",
    "    iter_corpus = corp[cf.iter:]\n",
    "    learning_rates = np.linspace(cf.eta, cf.min_eta, len(iter_corpus))\n",
    "    assert neg_sampler is not None, \"Give me a negative sampler!\"\n",
    "    iters_ = izip(count(cf.iter),\n",
    "                  sliding_window(iter_corpus, C=cf.C),\n",
    "                  z.partition(cf.C, neg_sampler),\n",
    "                  learning_rates,\n",
    "                 )\n",
    "    iters = ut.timeloop(iters_, **cf.term)\n",
    "\n",
    "    for i, (w, cont_), negsamp_lst, eta in iters:\n",
    "        cont = [x for x in cont_ if x != w] if w in cont_ else cont_\n",
    "        for c, negsamps in zip(cont, negsamp_lst):\n",
    "            if set([w, c]) & set(negsamps):\n",
    "                negsamps = [x for x in negsamps if x not in {w, c}]\n",
    "\n",
    "            sub_ixs = [w, c] + negsamps # list(negsamps)\n",
    "            Wsub = W[sub_ixs]\n",
    "            grad = ns_grad(Wsub)\n",
    "            gnorm = grad_norm(grad)\n",
    "            \n",
    "            if gnorm > 5:  # clip gradient\n",
    "                grad /= gnorm\n",
    "            W[sub_ixs] -= eta * grad    \n",
    "                \n",
    "        if i % 1000 == 0 and np.isnan(grad).any():\n",
    "            global grd\n",
    "            print('ruh roh!'); grd = grad\n",
    "            return\n",
    "\n",
    "    tdur = time.time() - st\n",
    "    print('{:.2f} mins'.format(tdur / 60))\n",
    "    cf2 = update(cf, norms=norms, gradnorms=gradnorms, iter=i+1)\n",
    "    cf2['accumsec'] += tdur\n",
    "    if not cf2.term:\n",
    "        DataFrame(W, index=vc).to_csv(os.path.join(cf2.dir, 'n{}_e{}.csv'.format(cf.N, cf.epoch)))\n",
    "        cf2['epoch'] += 1\n",
    "        cf2 = update(cf2, iter=0)\n",
    "    else:\n",
    "        print(i, 'iters')\n",
    "    # ping()\n",
    "    return W, cf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngsamp = neg_sampler_j(toki, cnfe.K)\n",
    "fast_opts = dict(ns_grad=ns_grad_jit, neg_sampler=ngsamp, sliding_window=sliding_window_jit)\n",
    "%lprun -T lp5.txt -s -f sgd sgd(W=We.copy(), corp=toki, cf=update(cnfe, term={'iters': 10000}), **fast_opts) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd(W=We.copy(), corp=toki, cf=update(cnfe, term={'iters': 10000}), **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -T lp.txt -s -f sgd sgd(W=We.copy(), corp=toki, cf=update(cnfe, term={'iters': 10000})) # ls[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rand_ixs = lambda W, n=8, axis=0: nr.randint(0, W.shape[axis], size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ixs = [ 1, 4, 7, 99, 486, 263, 924]\n",
    "w1, w2 = wtst.copy(), wtst.copy()\n",
    "# w1, w2 = wtst[ixs].copy(), wtst[ixs].copy()\n",
    "gr = ns_grad(w1[ixs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit ineg(w1, ixs, gr)\n",
    "%timeit inegp(w2, ixs, gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnfe = update(cnf, C=4, iter=0, term=dict(), N=100,  dir='cache/v12', epoch=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# tks = np.array(all_text.split())\n",
    "stoks, dropped = get_subsample(toks, thresh=THRESH)\n",
    "# assert 'Albus_Dumbledore' in stoks\n",
    "dv = WordVectorizer().fit(stoks)\n",
    "toki = [dv.vocabulary_[x] for x in stoks]\n",
    "vc = dv.feature_names_\n",
    "W = wut.init_w(len(vc), cnfe.N, seed=1)\n",
    "with open('txt.txt','w') as f:\n",
    "    f.write('\\n'.join(stoks))\n",
    "    \n",
    "We = W.copy()\n",
    "\n",
    "!say done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brown.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print('Epoch {}'.format(cnfe.epoch))\n",
    "    We2, cnfe = sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()), **fast_opts)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk; reload(nltk)\n",
    "from nltk.corpus import brown, reuters\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnfe = update(cnf, C=4, iter=0, term=dict(), N=100, dir='cache/v12', epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default\n",
    "\n",
    "size=100\n",
    "alpha=0.025\n",
    "window=5\n",
    "sample=0\n",
    "negative=0\n",
    "sg=1\n",
    "iter=4\n",
    "\n",
    "min_alpha=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gparams = dict(\n",
    "    size=120, # 80, #\n",
    "    alpha=0.025,\n",
    "    window=2,\n",
    "    sample=0,\n",
    "    negative=2,  #[5, 7, 10, 12, 15, 17], 0\n",
    "    sg=1,\n",
    "    iter=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ev(mod):\n",
    "    ans = mod.accuracy('src/questions-words.txt', restrict_vocab=10000)\n",
    "    sect = [d for d in ans if d['section'] == 'total']\n",
    "    return sum([1 for d in sect for _ in d['correct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time gmod = Word2Vec(brown.sents(), **update(gparams))\n",
    "ev(gmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def to_ints(wds):\n",
    "    le = LabelEncoder().fit(wds)\n",
    "    ints = le.transform(wds)\n",
    "    return list(ints), le\n",
    "\n",
    "def to_list_gen(f):\n",
    "    @wraps(f)\n",
    "    def f2(*a, **kw):\n",
    "        gen = f(*a, **kw)\n",
    "        return (list(x) for x in gen)\n",
    "    return f2\n",
    "\n",
    "def tokenize(wds):\n",
    "    alpha_re = re.compile(r'[A-Za-z]')\n",
    "    return [w for w in wds if alpha_re.search(w)]\n",
    "\n",
    "def prune_words(wds, keep_n_words=30000, min_counts=None):\n",
    "    if (keep_n_words is None) and (min_counts is None):\n",
    "        return wds\n",
    "    cts = Counter(wds)\n",
    "    if min_counts is not None:\n",
    "        return [w for w in wds if cts[w] >= min_counts]\n",
    "    elif keep_n_words is not None:\n",
    "        keeps = set(sorted(cts, key=cts.get, reverse=True)[:keep_n_words])\n",
    "        \n",
    "    return [w for w in wds if w in keeps]\n",
    "\n",
    "# c2 = prune_words(brown.words(), keep_n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toks, le = to_ints(nopunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def from_gensim_params(params, cnf, **upkw):\n",
    "    gsim2param_names = dict(negative='K', alpha='eta', size='N')\n",
    "    newparams = {pn: params[gn] for gn, pn in gsim2param_names.items()}\n",
    "    newparams['C'] = params['window'] * 2\n",
    "    cnf2 = update(cnf, **newparams, **upkw)\n",
    "    return cnf2\n",
    "\n",
    "cnff = Conf(from_gensim_params(gparams, cnfe, dir='cache/v13'))\n",
    "# cnff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = gparams\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class word2vec(object):\n",
    "    def __init__(self, words, cnf, neg_sampler=to_list_gen(neg_sampler_np),\n",
    "                 keep_n_words=None, min_counts=None, **sgd_kwds):\n",
    "        text = prune_words(tokenize(words), keep_n_words=keep_n_words, min_counts=min_counts)\n",
    "        \n",
    "        self.text = text\n",
    "        self.toks, self.le = to_ints(self.text)\n",
    "        self.cnf = update(cnf, term={})\n",
    "        self.W = wut.init_w(len(self.le.classes_), cnf.N)\n",
    "        self.sgd_kwds = sgd_kwds\n",
    "        self.neg_sampler = neg_sampler(self.toks, cnf.K)\n",
    "        \n",
    "    def run(self, term=None, **sgd_kwds):\n",
    "        cnf = self.cnf\n",
    "        if term:\n",
    "            cnf = update(cnf, term=term)\n",
    "        res = sgd(W=self.W.copy(), corp=self.toks, neg_sampler=self.neg_sampler,\n",
    "                  cf=cnf, vc=self.le.classes_, **z.merge(self.sgd_kwds, sgd_kwds))\n",
    "        self.W, self.cnf = res\n",
    "        \n",
    "    @property\n",
    "    def df(self):\n",
    "        return DataFrame(self.W.copy(), index=self.le.classes_)\n",
    "\n",
    "modf = word2vec(brown.words(), cnff, neg_sampler=neg_sampler_j, keep_n_words=None, min_counts=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('cache/txt.txt','w') as f:\n",
    "    f.write('\\n'.join(modf.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_opts = dict(ns_grad=ns_grad_jit, sliding_window=sliding_window_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(4):   \n",
    "    modf.run(term={}, **fast_opts)  # 'iters': 10000\n",
    "!say done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modf.run(term={}, **fast_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modf.cnf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def partition(pred, iterable):\n",
    "    'Use a predicate to partition entries into false entries and true entries'\n",
    "    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9\n",
    "    t1, t2 = it.tee(iterable)\n",
    "    return it.filterfalse(pred, t1), ifilter(pred, t2)\n",
    "\n",
    "with open('src/questions-words.txt', 'r') as f:\n",
    "    qlns = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('src/questions-words.txt', 'r') as f:\n",
    "    qlns = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modf.df = DataFrame(modf.W.copy(), index=modf.le.classes_)\n",
    "W2 = modf.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sections, qs = partition(lambda s: not s.startswith(':'), qlns)\n",
    "qs = list(qs)\n",
    "# allwds = set(modf.df.index)\n",
    "# del allwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cts = Series(Counter(modf.text)) # / len(modf.text)\n",
    "assert (cts.index == modf.le.classes_).all()\n",
    "keeps = cts >= 5\n",
    "Wk = modf.df.divide(norm(modf.df, axis=1), axis=0)\n",
    "Wsmall = Wk[keeps]\n",
    "assert (norm(Wk, axis=1).round(4) == 1).all(), 'Not normalized'\n",
    "# assert (Wk.sum(axis=1).round(4) == 1).all(), 'Not normalized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -s -f eval eval_qs(Wk, lim=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_vec(w, W):\n",
    "    if isinstance(w, str):\n",
    "        return W.ix[w]\n",
    "    return w\n",
    "\n",
    "def to_vec2(w, W, wd2row=None):\n",
    "    if isinstance(w, str):\n",
    "        return W.values[wd2row[w]]\n",
    "    return w\n",
    "\n",
    "neg = lambda x: -x\n",
    "\n",
    "def combine(plus=[], minus=[], W=None, wd2row=None):\n",
    "    to_vec_ = partial(to_vec, W=W)\n",
    "    vecs = map(to_vec_, plus) + map(z.comp(neg, to_vec_), minus)\n",
    "    v = sum(vecs) / len(vecs)\n",
    "    return v / norm(v)\n",
    "\n",
    "def combine2(plus=[], minus=[], W=None, wd2row=None):\n",
    "    # plus_ix = [wd2row[p] for p in plus]\n",
    "    # minus_ix = [wd2row[p] for p in minus]\n",
    "    ixs = [wd2row[p] for p in plus + minus]\n",
    "    vecs1 = Wk.values[ixs]\n",
    "    to_vec_ = partial(to_vec2, W=W, wd2row=wd2row)\n",
    "    \n",
    "    vecs = map(to_vec_, plus) + map(z.comp(neg, to_vec_), minus)\n",
    "    vecs = np.array(vecs)\n",
    "    return combine_(vecs)\n",
    "    v = sum(vecs) / len(vecs)\n",
    "    return v / norm(v)\n",
    "\n",
    "\n",
    "@nopython\n",
    "def combine_(vecs):\n",
    "#     v = sum(vecs) / len(vecs)\n",
    "    v = np.sum(vecs, axis=0) / len(vecs)\n",
    "    return v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = np.array(vecs)\n",
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -s -f combine2 combine2(plus=[b, c], minus=[a], W=Wk, wd2row=wd2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wvec = combine(plus=[b, c], minus=[a], W=Wk)\n",
    "wvec[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mk_wd2row = lambda W: dict(zip(W.index, count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wd2ix = mk_wd2row(Wk)\n",
    "wvec2 = \n",
    "wvec2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit combine(plus=[b, c], minus=[a], W=Wk)\n",
    "%timeit combine2(plus=[b, c], minus=[a], W=Wk, wd2row=wd2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine2(plus=[b, c], minus=[a], W=Wk, wd2row=wd2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(wvec == wvec2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def index(wd2ix):\n",
    "    \n",
    "    return wd2ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wds, arr, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index({'abc': 123})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wv = Wk.values\n",
    "f1 = lambda: Wk.ix['youngster']\n",
    "f2 = lambda: Wk.iloc[-18]\n",
    "f3 = lambda: Wv[-18]\n",
    "assert np.allclose(f1(), f3())\n",
    "assert np.allclose(f1(), f2())\n",
    "\n",
    "%timeit f1()\n",
    "%timeit f2()\n",
    "%timeit f3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval(q, Wall=None, Wsmall=None, Wnorm=None, allwds=None):\n",
    "    qwds = a, b, c, d = q.split()\n",
    "    if allwds is None:\n",
    "        print('Warning: precalculate `allwds`')\n",
    "        allwds = set(Wall.index)\n",
    "    missing_wds = {a, b, c} - allwds\n",
    "    if missing_wds:\n",
    "        # print(u'\\u2639', end='')\n",
    "#         print(u'.', end='')\n",
    "        return False\n",
    "    wvec = combine(plus=[b, c], minus=[a], W=Wall)\n",
    "    [closest] = wut.get_closestn(wd=wvec, W=Wsmall, n=1, exclude=[a, b, c], just_word=1)\n",
    "    ans = closest == d\n",
    "        \n",
    "    return ans\n",
    "\n",
    "        \n",
    "def eval_qs(Wsmall, Wall, lim=None):\n",
    "    Wn = np.linalg.norm(Wsmall, axis=1)\n",
    "    allwds = set(Wall.index)\n",
    "    sm = 0\n",
    "    for ql in qs[:lim]:\n",
    "        print(list(allwds)[:5])\n",
    "        e = eval(ql, Wall=Wall, Wsmall=Wsmall, Wnorm=Wn, allwds=allwds)\n",
    "        if e:\n",
    "#             print('!', end='')\n",
    "            print(ql, end=':')\n",
    "            sm += 1\n",
    "    return sm\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wk.ix??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wk._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %lprun -s -f eval eval_qs(Wk, lim=500)\n",
    "%lprun -s -f combine eval(ql, Wall=Wk, Wsmall=Wk, Wnorm=norm(Wk, axis=1), allwds=set(Wk.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %lprun eval(ql, Wall=Wk, Wsmall=Wk, Wnorm=norm(Wk, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = eval_qs(Wk, Wk, lim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod.syn0norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b, c, d = 'boy', 'girl', 'brothers', 'sisters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = np.linalg.norm\n",
    "reload(wut);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "closests = wut.get_closestn.closests\n",
    "print(len(closests))\n",
    "closests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm(Wk, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wvec = combine(plus=[b, c], minus=[a], W=Wk)\n",
    "cl = wut.get_closestn(wd=wvec, W=Wk, Wnorm=None, n=1, exclude=[a, b, c], just_word=1)\n",
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wut.cdist(wvec, Wk)[Wk.index.isin([a, b, c])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sis = Wsmall.ix['sisters']\n",
    "(sis @ wvec) / (norm(sis) * norm(wvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ga, gb, gc, gd = gmod.vocab[a], gmod.vocab[b], gmod.vocab[c], gmod.vocab[d],\n",
    "ga, gb, gc, gd = gmod.syn0norm[ga.index], gmod.syn0norm[gb.index], gmod.syn0norm[gc.index], gmod.syn0norm[gd.index]\n",
    "gv = (gb + gc - ga) / 3\n",
    "gv /= norm(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norms = gmod.syn0norm @ gv\n",
    "isort = np.argsort(norms)[::-1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[gmod.index2word[i] for i in isort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norms[isort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod.syn0norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_closests?\n",
    "get_closests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(Wk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsc = gmod.accuracy('src/questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsc_dct = {doc.pop('section'): doc for doc in gsc}\n",
    "gfound = {w for set_ in gsc_dct['total']['correct'] for w in set_[:3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b, c, gmod.most_similar(positive=[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod.most_similar(positive=[b, c], negative=[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsc_dct['family']['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in gsc:\n",
    "    print(doc['section'], end=': ')\n",
    "    print(len(doc['correct']))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(gsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnfs = update(cnfe, dir='cache/slow')\n",
    "mod = word2vec(brown.words(), cnfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time mod.run(term={})  # 'iters': 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gensim benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim_sents = brown.sents() # [s.orth_.split() for s in atks.sents]\n",
    "\n",
    "param_vals = dict(\n",
    "    sample=[1e-3, 5e-3, 1e-2, 5e-2, .1, .15, 0],\n",
    "    negative=range(0, 8),  #[5, 7, 10, 12, 15, 17],\n",
    "    window=range(2, 20),\n",
    "    sg=[0, 1],\n",
    "    size=np.arange(1, 40) * 4,\n",
    "    alpha=[0.05, 0.025, 0.01, 0.005],\n",
    "    iter=range(1, 5),\n",
    "    \n",
    ")\n",
    "\n",
    "param_lst = ['alpha', 'iter', 'negative', 'sample', 'sg', 'size', 'window']\n",
    "assert sorted(param_vals) == param_lst, 'Need to update param_lst'\n",
    "\n",
    "cs = param_lst + ['score']\n",
    "to_param_dct = lambda xs: OrderedDict(zip(param_lst, xs))\n",
    "from_param_dct = lambda dct, cs=param_lst: [dct[c] for c in cs]\n",
    "\n",
    "# param_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_closests(to_param_dct([0.025, 4.0, 0.0, 0.0, 1.0, 80.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_param_dct([0.025, 4.0, 0.0, 0.0, 1.0, 80.0, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_closest(k, v, poss=param_vals):\n",
    "    return min(poss[k], key=lambda x: abs(v - x))\n",
    "\n",
    "def get_closests(dct, poss=param_vals):\n",
    "    return {k: get_closest(k, v, poss=poss) for k, v in dct.items()}\n",
    "\n",
    "def param_gen(params, n_iters=None):\n",
    "    counter = count() if n_iters is None else range(n_iters)\n",
    "    for i in counter:\n",
    "        yield {k: nr.choice(v) for k, v in param_vals.items()}\n",
    "\n",
    "def run_model(n=None, perfs=None, lfname='cache/log.csv'):\n",
    "    pg = param_gen(param_vals, n)\n",
    "\n",
    "    for params in pg:\n",
    "        st = time.time()\n",
    "        model = Word2Vec(sentences=gensim_sents, workers=4, **params)\n",
    "        perf = dict(params)\n",
    "        perf['score'] = score(model)\n",
    "        perfs.append(perf)\n",
    "        print('time: {:.2f}'.format(time.time() - st))\n",
    "      \n",
    "        if not os.path.exists(lfname):\n",
    "            mode, header = 'w', True\n",
    "        else:\n",
    "            mode, header = 'a', False\n",
    "        with open(lfname, mode) as f:\n",
    "            DataFrame([perf])[cs].to_csv(f, header=header, sep='\\t')\n",
    "        sys.stdout.flush()\n",
    "    return perfs\n",
    "\n",
    "def score(model, restrict_vocab=10000):\n",
    "    acc = model.accuracy('src/questions-words.txt', restrict_vocab=restrict_vocab)\n",
    "    [tot] = [d for d in acc if d['section'] == 'total']\n",
    "    print(len(tot['correct']), '/', len(tot['incorrect']) + len(tot['correct']), end=' ')\n",
    "    return len(tot['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_ex = next(param_gen(param_vals, None))\n",
    "ex_vals = map(itg(1), sorted(param_ex.items()))\n",
    "to_param_dct(ex_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = minimize(rosen, x0, method='nelder-mead',\n",
    "...                options={'xtol': 1e-8, 'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_func(paramlist):\n",
    "    print(paramlist)\n",
    "    param_dct_ = to_param_dct(paramlist)\n",
    "    param_dct = {k: get_closest(k, v) for k, v in param_dct_.items()}\n",
    "    print(param_dct)\n",
    "    model = Word2Vec(sentences=gensim_sents, workers=4, **param_dct)\n",
    "    return score(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perfs = []\n",
    "perfs = run_model(n=None, perfs=perfs)\n",
    "## End Gensim benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perfs[:4][cs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Series(list(cts.values())).value_counts(normalize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modf.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z.merge({2: 4}, {2: 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(**kw):\n",
    "    print(kw)\n",
    "    \n",
    "f(a=2, **{'a': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  # neg_sampler=ngsamp, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngsamp = neg_sampler_j(toki, cnfe.K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%lprun -T lp5.txt -s -f sgd sgd(W=We.copy(), corp=toki, cf=update(cnfe, term={'iters': 10000}), **fast_opts) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnfe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmod.most_similar('politician')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()), **fast_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.allclose(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ineg(w1, ixs, gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inegp(w2, ixs, gr)\n",
    "np.allclose(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtst[ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtst = wut.init_w(1000, 50)\n",
    "wsub = wtst[:8].copy()\n",
    "gr = ns_grad(wsub)\n",
    "# gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ineg1(wsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad, grad2 = grd, grd2\n",
    "Wsub = W2[[w, c] + list(negsamps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(approx_grad(Wsub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(Wsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(ns_grad(W2[[w, c] + list(negsamps)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(vc.__getitem__, [c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(vc.__getitem__, negsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negsamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del w, cont, c, negsamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(grad, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc[19276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toki[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgdpart = partial(sgd, W=W.copy(), corp=toki, cf=update(cnfe, term={'iters': 1}))\n",
    "_ = sgdpart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradnorm_vec = np.linalg.norm(grad, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if np.linalg.norm(gradnorm_vec) > 5:\n",
    "    grad = grad / gradnorm_vec[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradnormed = np.divide(grad, gradnorm)\n",
    "gradnormed = grad / gradnorm[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(gradnormed, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradnormed.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "7.12\n",
    "  1085718\n",
    "6.00742\n",
    "-> 684739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd(W=W.copy(), corp=toki, cf=update(cnf, term={'iters': 1}));\n",
    "%prun -qD prof.txt sgd(W=W.copy(), corp=toki, cf=update(cnf, term={'iters': 5000})) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We = pd.read_csv('cache/v9/n15_e26.csv', index_col=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(*zip(*gns), alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print('Epoch {}'.format(cnfe.epoch))\n",
    "    We2, cnfe = sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "We2 - We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "W2, cnf2 = sgd(W=W.copy(), corp=toki, cf=update(cnf, term={'mins': 60})) # ls[:20]\n",
    "sw = sliding_window(toki, C=cnf.C)\n",
    "# 799509 iters\n",
    "# CPU times: user 12min 29s, sys: 4.47 s, total: 12min 34s\n",
    "# Wall time: 12min 35s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "W3, cnf3 = sgd(W=W2.copy(), corp=toki, cf=update(cnf2, iter=0)) # ls[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "W3, cnf2 = sgd(W=W2.copy(), corp=toki, cf=update(cnf, term={'mins': 5})) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W3, cnf3 = sgd(W=W3.copy(), corp=toki, cf=update(cnf3, term={'mins': 5}), ) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wd = DataFrame(W2, index=vc)\n",
    "wd.to_csv('cache/v2/n6_e1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd.mean(axis=1).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wd[10:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Series(cnfe.norms).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, x in enumerate([-1, 1,6,8]):\n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf2.norms[48000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "gnrms = Series(cnfe.gradnorms)\n",
    "gnrms.plot()\n",
    "pd.rolling_mean(gnrms, 20).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update equation\n",
    "$$\n",
    "    \\frac{\\partial E}\n",
    "         {\\partial \\boldsymbol v_{w_j}' ^T \\boldsymbol h}\n",
    "         = \\sigma(\\boldsymbol v_{w_j}' ^T \\boldsymbol h) -t_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyze word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
