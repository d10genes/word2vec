{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "A difficulty of working with text data is that using each word of a large vocabulary as a feature requires working with large dimensions. While sparse matrix data structures allow for tractable manipulation of these vectors, there are whole classes of algorithms that do not work well or at all on sparse high dimensional data. \n",
    "\n",
    "A recent development called word2vec allows for words to be represented as dense vectors of much smaller dimensions (on the order of $\\mathbb{R}^{100}$). This algorithm yields representations of words such that words appearing in similar contexts will lie close to one another in this low dimensional vector space. Another interesting feature of this algorithm is that the representation does a good job at inferring analogies. [TODO: ex?]\n",
    "\n",
    "At a high level, the skip-gram flavor of this algorithm looks at a word and its surrounding  words, and tries to maximize the probability that the word's vector representation predicts those actual words occurring around it. If it is trained on the phrase *the quick brown fox jumps*, the word2vec representation of the word *brown* would yield a high dot product with the vectors for the words *the, quick, fox* and *jumps*.\n",
    "\n",
    "## Speed\n",
    "The go-to word2vec implementation in python (and, apparently in general) seems to be the very efficient cython-based [gensim](https://github.com/piskvorky/gensim).\n",
    "\n",
    "In this [notebook TODO](link), I first implement the word2vec algorithm, and then try to speed it up using [numba](numba.pydata.org), a numeric JIT compiler that utilizes LLVM and supports a subset of python and numpy.\n",
    "\n",
    "[summarize results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var csc = IPython.keyboard_manager.command_shortcuts\n",
    "csc.add_shortcut('Ctrl-k','ipython.move-selected-cell-up')\n",
    "csc.add_shortcut('Ctrl-j','ipython.move-selected-cell-down')\n",
    "csc.add_shortcut('Shift-m','ipython.merge-selected-cell-with-cell-after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from project_imports import *\n",
    "import utils as ut; reload(ut);\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import scipy as sp\n",
    "# sp.sparse.csr_matrix.__matmul__ = sp.sparse.csr_matrix.dot\n",
    "# import numpy as np\n",
    "# from numpy import exp, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wordvec_utils as wut; reload(wut);\n",
    "from wordvec_utils import Cat, WordVectorizer\n",
    "from voluptuous import Schema, ALLOW_EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Subsample\n",
    "# THRESH = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective functions\n",
    "\n",
    "The standard skip-gram objective function comes from taking the softmax probability of each of the actual context words dotted with the input ($u_{c,j^*_c}$) and multiplying them together:\n",
    "\n",
    "\\begin{align}\n",
    "E & = -\\log \\prod_{c=1} ^{C}\n",
    "    \\frac {\\exp (u_{c,j^*_c})}\n",
    "          {\\sum_{j'=1} ^ V \\exp(u_{j'})} \\\\\n",
    "  & = -\\sum^C_{c=1} u_{j^*_c} + C \\cdot \\log \\sum ^ V _{j'=1} \\exp(u_j')\n",
    "\\end{align}\n",
    "\n",
    "See [word2vec Parameter Learning Explained](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf) for a detailed explanation.\n",
    "Though I ended up using a different objective called negative sampling, I kept this original objective below as `skipgram_likelihood`, written as a nested function since autograd requires single-argument functions to differentiate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram_likelihood(wi, cwds, dv=None):\n",
    "    wix = dv.get(wi)\n",
    "    cixs = dv.get(cwds)\n",
    "    C = len(cwds)\n",
    "\n",
    "    def logloss(Wall):\n",
    "        W1, W2 = Cat.split(Wall)\n",
    "        h = W1[wix, :]  # ∈ ℝⁿ\n",
    "        u = np.dot(h, W2)  # u[1083] == 427  ∈ ℝⱽ\n",
    "        ucs = u[cixs]\n",
    "        return -np.sum(ucs) + C * np.log(np.sum(np.exp(u)))\n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value of `logloss` should pretty clearly resembles the equation above.\n",
    " \n",
    "To train word2vec, the gradient is actually more important than the actual function, but for the standard skip-gram architecture, both are relatively straightforward. In an earlier iteration, I used the python [autograd](https://github.com/HIPS/autograd) library to try and cheat having to manually write the derivative function. Unfortunately, it quickly became apparent that this was the chief bottleneck of the gradient descent routine, so I resorted to manually coding the derivatives like an animal.\n",
    "\n",
    "# Negative sampling\n",
    "## Gradient\n",
    "\n",
    "After reading a bit more about word2vec, I found out about an extension to the skip-gram model called negative sampling that efficiently generates better word vectors. The basic idea is that in addition to training a word vector with $C$ words that *do* appear around it, the vector should also be trained with words randomly chosen from the rest of the text, as negative examples of what the vector should *not* predict in its context. \n",
    "\n",
    "As a side-note to keep up with the notation taken from [word2vec Parameter Learning Explained](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf), the term $\\boldsymbol v_{w_O}'$ refers to a word vector ($\\boldsymbol v$) that is from the output vector matrix ($\\boldsymbol v'$) representing an output word $w_O$. Notation aside, the gradient for the negative sampling extension is relatively straightforward. For each true context word vector $\\boldsymbol v_{w_O}'$ appearing close to the input word vector $h$, we'll draw $K$ word vectors $\\boldsymbol v_{w_i}$ at random from the corpus. The objective is computed by adding the log of the sigmoid of the word vector dotted with either the true or false context word (the negative samples are negated): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "E = -\\log \\sigma(\\boldsymbol v_{w_O}' ^T \\boldsymbol h)\n",
    "    - \\sum^K _{i=1} \\log \\sigma (-\\boldsymbol v_{w_i}' ^T \\boldsymbol h)\n",
    "$$\n",
    "\n",
    "The gradient with respect to $\\boldsymbol v_{w_j}' ^T \\boldsymbol h$ is then as follows, where $t_j$ is an indicator for whether $w_j$ actually appears in the context:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial E}\n",
    "         {\\partial \\boldsymbol v_{w_j}' ^T \\boldsymbol h}\n",
    "         = \\sigma(\\boldsymbol v_{w_j}' ^T \\boldsymbol h) -t_j.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from autograd.numpy import exp, log\n",
    "# from builtins import zip as izip, range as xrange\n",
    "# import negsamp_grad; reload(negsamp_grad);\n",
    "# import numpy as np\n",
    "\n",
    "from negsamp_grad import ns_grad as ns_grad_jit\n",
    "from autograd import numpy as npa, grad\n",
    "\n",
    "from numba import jit\n",
    "nopython = jit(nopython=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getNall = lambda W: W.shape[1] // 2\n",
    "gen_labels = lambda negsamps: [1] + [0] * len(negsamps)\n",
    "sig = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "def get_vecs1_(Wall, w_ix: int=0, vo_ix: [int]=1, negsamp_ixs: [int]=None):\n",
    "    if negsamp_ixs is None:\n",
    "        negsamp_ixs = range(2, len(Wall))\n",
    "    N = getNall(Wall)\n",
    "    h = Wall[w_ix, :N]  # ∈ ℝⁿ\n",
    "    vwo = Wall[vo_ix, N:]\n",
    "    negsamps = Wall[negsamp_ixs, N:]\n",
    "    return h, vwo, negsamps\n",
    "\n",
    "\n",
    "def get_vecs1(Wsub):\n",
    "    N = getNall(Wsub)\n",
    "    h = Wsub[0, :N]  # ∈ ℝⁿ\n",
    "    vwo = Wsub[1, N:]\n",
    "    negsamps = Wsub[2:, N:]\n",
    "    return h, vwo, negsamps\n",
    "\n",
    "def ns_loss_grads(h: 'v[n]', vout: '[v[n]]', label: 'v[n]'):\n",
    "    dot = sig(vout @ h) - label\n",
    "    return dot * vout, dot * h\n",
    "\n",
    "def ns_grad(Wsub):\n",
    "    # global hgrad, vgrad, Wsub, N\n",
    "    h, vwo, negsamps = get_vecs1(Wsub)\n",
    "    N = getNall(Wsub)\n",
    "    Wsub_grad = np.zeros(Wsub.shape)\n",
    "\n",
    "    for i, vout, label in izip(count(1), it.chain([vwo], negsamps), gen_labels(negsamps)):\n",
    "        hgrad, vgrad = ns_loss_grads(h, vout, label)\n",
    "        Wsub_grad[0, :N] += hgrad\n",
    "        Wsub_grad[i, N:] += vgrad\n",
    "    return Wsub_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "The following gradient checking functionality based on [the UFLDL tutorial](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/) can be used to ensure that autograd is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ns_loss(h, vwo, vwi_negs):\n",
    "    \"\"\"This should be called on the subset of the matrix (Win || Wout')\n",
    "    determined by row indices `wi, win_ix, negwds`.\n",
    "    \"\"\"\n",
    "    negsum = 0\n",
    "    for j in xrange(len(vwi_negs)):\n",
    "        negsum += np.log(sig(-vwi_negs[j] @ h))\n",
    "        \n",
    "    return -np.log(sig(vwo @ h)) - negsum\n",
    "\n",
    "\n",
    "def ns_loss_vec(h, vwo, vwi_negs):\n",
    "    \"\"\"This should be called on the subset of the matrix (Win || Wout')\n",
    "    determined by row indices `wi, win_ix, negwds`.\n",
    "    \"\"\"\n",
    "    return -np.log(sig(vwo @ h)) - np.sum(np.log(sig(-vwi_negs @ h )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def J(Wsub, loss=ns_loss):\n",
    "    N = getNall(Wsub)\n",
    "    h, vwo, vwi_negs = get_vecs1(Wsub)\n",
    "    # h, vwo, vwi_negs = Wsub[0, :N], Wsub[1, N:], Wsub[range(2, len(Wsub)), N:]\n",
    "    return loss(h, vwo, vwi_negs)\n",
    "    return ns_loss_vec(h, vwo, vwi_negs)\n",
    "\n",
    "def check_grad_(W, i: int=None, j: int=None, eps=1e-6, J: 'Callable'=None):\n",
    "    \"From eqn at http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/\"\n",
    "    Wneg, Wpos = W.copy(), W.copy()\n",
    "    Wneg[i, j] -= eps\n",
    "    Wpos[i, j] += eps\n",
    "    return (J(Wpos) - J(Wneg)) / (2 * eps)\n",
    "\n",
    "def approx_grad(W, J=J):\n",
    "    n, m = W.shape\n",
    "    grad = np.zeros_like(W)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            grad[i, j] = check_grad_(W, i=i, j=j, eps=1e-6, J=J)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def siga(x):\n",
    "    return 1 / (1 + npa.exp(-x))\n",
    "\n",
    "def mk_ns_loss_a(N):\n",
    "    def ns_loss_a(Wsub):\n",
    "        h = Wsub[0, :N]\n",
    "        vwo = Wsub[1, N:]\n",
    "        vwi_negs = Wsub[2:, N:]\n",
    "        vwo_h = npa.dot(vwo, h)\n",
    "        vwi_negs_h = npa.dot(vwi_negs, h)\n",
    "        return  -npa.log(siga(vwo_h)) - npa.sum(npa.log(siga(-vwi_negs_h)))\n",
    "    return ns_loss_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_ = 50; W = wut.init_w(1000, N_, seed=1); Wsub = W[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_check = lambda x: approx_grad(x, partial(J, loss=ns_loss))\n",
    "np_vec_check = lambda x: approx_grad(x, partial(J, loss=ns_loss_vec))\n",
    "ns_grad_auto = grad(mk_ns_loss_a(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_close(f, grd=ns_grad(Wsub)):\n",
    "    grd2 = f(Wsub)\n",
    "    print('√ Diff: {}'.format(np.linalg.norm(grd - grd2)))\n",
    "    return np.allclose(grd, grd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert grad_close(np_check)\n",
    "assert grad_close(np_vec_check)\n",
    "assert grad_close(ns_grad_auto)\n",
    "assert grad_close(ns_grad)\n",
    "assert grad_close(ns_grad_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np_check(Wsub)\n",
    "%timeit np_vec_check(Wsub)\n",
    "%timeit ns_grad_auto(Wsub)\n",
    "%timeit ns_grad(Wsub)\n",
    "%timeit ns_grad_jit(Wsub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw negative samples\n",
    "\n",
    "As a foreshadowing of performance bottlenecks that my original implementation ran into, I have a few versions of a function that chooses words from the text at random, that increase in performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigram(txt, pow=.75):\n",
    "    \"Unigram^(3/4) model\"\n",
    "    cts = Series(Counter(txt))\n",
    "    \n",
    "    # If txt is integers, fill in missing values (likely for unknown token)\n",
    "    # with 0 probability to reliably use index to identify token\n",
    "    int_txt = cts.index.dtype == int\n",
    "    if int_txt:\n",
    "        missing_tokens = set(range(cts.index.max())) - set(cts.index)\n",
    "        for msg in missing_tokens:\n",
    "            cts.loc[msg] = 0\n",
    "        cts = cts.sort_index()\n",
    "        \n",
    "    N = len(txt)\n",
    "    ctsdf = ((cts / cts.sum()) ** pow).reset_index(drop=0)\n",
    "    ctsdf.columns = ['Word', 'Prob']\n",
    "    if int_txt:\n",
    "        assert (ctsdf.Word == ctsdf.index).all()\n",
    "    return ctsdf\n",
    "\n",
    "\n",
    "def neg_sampler_pd(xs, K, pow=.75):\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    for seed in count():\n",
    "        yield ug.Word.sample(n=K, weights=ug.Prob, random_state=seed, replace=True)\n",
    "        \n",
    "        \n",
    "def neg_sampler_np(xs, K, cache_len=1000, use_seed=False, pow=.75):\n",
    "    \"Faster neg. sampler without the pandas overhead\"\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    p = ug.Prob.values / ug.Prob.sum()\n",
    "    a = ug.Word.values\n",
    "\n",
    "    for seed in count():\n",
    "        if use_seed:\n",
    "            nr.seed(seed)\n",
    "        Wds = nr.choice(a, size=(cache_len, K), p=p)\n",
    "        for wds in Wds:\n",
    "            yield wds\n",
    "            \n",
    "            \n",
    "def neg_sampler_np_l(xs, K, cache_len=1000, pow=.75):\n",
    "    \"Faster neg. sampler without the pandas overhead\"\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    p = ug.Prob.values / ug.Prob.sum()\n",
    "    a = list(ug.Word.values)\n",
    "\n",
    "    @nopython\n",
    "    def sample_():\n",
    "        while 1:\n",
    "            Wds = nr.choice(a, size=(cache_len, K), p=p)\n",
    "            for i in xrange(len(Wds)):\n",
    "                yield Wds[i]\n",
    "    return sample_\n",
    "\n",
    "\n",
    "def neg_sampler_j(xs, K, pow=.75):\n",
    "    ug = unigram(xs, pow=pow)\n",
    "    cum_prob = ug.Prob.cumsum() / ug.Prob.sum()\n",
    "    return neg_sampler_j_(cum_prob.values, K)\n",
    "\n",
    "@nopython\n",
    "def neg_sampler_j_(cum_prob, K):\n",
    "    while 1:\n",
    "        l = []\n",
    "        for i in xrange(K):\n",
    "            l.append(wut.bisect_left_jit(cum_prob, nr.rand()))\n",
    "        yield l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "gen = sample_(ug.Cum_prob.values, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check distributions\n",
    "\n",
    "And just as a sanity check that the different implementations do the same thing, I randomly generate words according to how frequently occur in the text with each of the samplers and scatter-plot them against each other, hoping that they mostly lie on $y=x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "some_text = ut.take(brown.words(), int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "smtok = le.fit_transform(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_jit = neg_sampler_j(smtok, 8)\n",
    "gen_np = neg_sampler_np(smtok, 8)\n",
    "gen_pd = neg_sampler_pd(smtok, 8)\n",
    "\n",
    "next(gen_jit); next(gen_np); next(gen_pd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "%time csp = Series(Counter(x for xs in it.islice(gen_pd, n // 100) for x in xs))\n",
    "%time csnp = Series(Counter(x for xs in it.islice(gen_np, n) for x in xs))\n",
    "%time csj = Series(Counter(x for xs in it.islice(gen_jit, n) for x in xs))\n",
    "\n",
    "ug = unigram(smtok, pow=.75)\n",
    "cts = DataFrame({'Numba': csj, 'Numpy': csnp, 'Pandas': csp}).fillna(0)\n",
    "probs = cts / cts.sum()\n",
    "probs['Probs'] = ug.Prob / ug.Prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_dist(xcol=None, subplt=None):\n",
    "    plt.subplot(subplt)\n",
    "    probs.plot(x=xcol, y='Probs', ax=plt.gca(), kind='scatter', alpha=.25)\n",
    "    _, xi = plt.xlim(None)\n",
    "    _, yi = plt.ylim(0, None)\n",
    "    end = min(xi, yi)\n",
    "    plt.plot([0, end], [0, end], alpha=.2)\n",
    "    \n",
    "plt.figure(figsize=(16, 10))\n",
    "plot_dist(xcol='Numba', subplt=131)\n",
    "plot_dist(xcol='Numpy', subplt=132)\n",
    "plot_dist(xcol='Pandas', subplt=133)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window\n",
    "It turned out another significant bottleneck of the gradient descent routine was the sliding window code that iterates over the entire corpus, yielding a word and its context words at each point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sliding_window(xs, C=4, start_pos=0):\n",
    "    \"\"\"Iterates through corpus, yielding input word\n",
    "    and surrounding context words\"\"\"\n",
    "    #assert isinstance(xs, list)\n",
    "    winsize = C // 2\n",
    "    N = len(xs)\n",
    "    for i, x in enumerate(xs, start_pos):\n",
    "        ix1 = max(0, i-winsize)\n",
    "        ix2 = min(N, i+winsize+1)\n",
    "        yield x, xs[ix1:i] + xs[i + 1:ix2]\n",
    "\n",
    "\n",
    "@nopython\n",
    "def bounds_check_window(i, xs: [int], winsize, N):\n",
    "    x = xs[i]\n",
    "    ix1 = max(0, i-winsize)\n",
    "    ix2 = min(N, i+winsize+1)\n",
    "    return x, xs[ix1:i] + xs[i + 1:ix2]\n",
    "\n",
    "\n",
    "@nopython\n",
    "def sliding_window_jit(xs, C=4):\n",
    "    \"\"\"Iterates through corpus, yielding input word\n",
    "    and surrounding context words\"\"\"\n",
    "    winsize = C // 2\n",
    "    N = len(xs)\n",
    "    for i in xrange(winsize):\n",
    "        yield bounds_check_window(i, xs, winsize, N)\n",
    "    for i in xrange(winsize, N-winsize):\n",
    "        context = []\n",
    "        for j in xrange(i-winsize, i+winsize+1):\n",
    "            if j != i:\n",
    "                context.append(xs[j])\n",
    "        yield xs[i], context  # xs[i-winsize:i] + xs[i + 1:i+winsize+1]\n",
    "    for i in xrange(N-winsize, N):\n",
    "        yield bounds_check_window(i, xs, winsize, N)\n",
    "      \n",
    "@nopython\n",
    "def concat(a, b):\n",
    "    na = len(a)\n",
    "    n = na + len(b)\n",
    "    c = np.empty(n, dtype=a.dtype)\n",
    "    for i in xrange(na):\n",
    "        c[i] = a[i]\n",
    "    for i in xrange(len(b)):\n",
    "        c[i + na] = b[i]\n",
    "    return c\n",
    "\n",
    "@nopython\n",
    "def bounds_check_window_arr(i, xs: np.array, winsize, N):\n",
    "    x = xs[i]\n",
    "    ix1 = max(0, i-winsize)\n",
    "    ix2 = min(N, i+winsize+1)\n",
    "    return x, concat(xs[ix1:i], xs[i + 1:ix2])\n",
    "\n",
    "@nopython\n",
    "def sliding_window_jit_arr(xs, C=4):\n",
    "    \"\"\"Iterates through corpus, yielding input word\n",
    "    and surrounding context words\"\"\"\n",
    "    winsize = C // 2\n",
    "    N = len(xs)\n",
    "    for i in xrange(winsize):\n",
    "        yield bounds_check_window_arr(i, xs, winsize, N)\n",
    "    for i in xrange(winsize, N-winsize):\n",
    "        context = np.empty(C, dtype=np.int64)\n",
    "        for ci in xrange(winsize):\n",
    "            context[ci] = xs[i - winsize + ci]\n",
    "            context[winsize + ci] = xs[i + 1 + ci]\n",
    "#             if j != i:\n",
    "#                 context[ci] = xs[i - winsize + ci]\n",
    "        yield xs[i], context  # xs[i-winsize:i] + xs[i + 1:i+winsize+1]\n",
    "    for i in xrange(N-winsize, N):\n",
    "        yield bounds_check_window_arr(i, xs, winsize, N)\n",
    "\n",
    "samp_toks = nr.randint(0, 1e6, size=100005)\n",
    "samp_toksl = list(samp_toks)\n",
    "list(sliding_window_jit(samp_toksl[:100]))\n",
    "run_window = lambda f, toks=samp_toksl: list(f(toks))\n",
    "\n",
    "%timeit run_window(sliding_window)\n",
    "%timeit run_window(sliding_window_jit)\n",
    "%timeit run_window(sliding_window_jit_arr, toks=samp_toks)\n",
    "# assert run_window(sliding_window) == run_window(sliding_window_jit) == run_window(sliding_window_jit_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "- conf\n",
    "For the gradient descent routine, I'm passing all the hyper-parameters through a configuration dictionary, validated by [voluptuous](https://pypi.python.org/pypi/voluptuous). This allows specification of things like the context window size, learning rate, number of negative samples and size of the word vectors. Check the utility file where it's defined for details on the parameter significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordvec_utils import Dict, Num, even, orig_type, update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils as ut; reload(ut);\n",
    "from voluptuous import ALLOW_EXTRA\n",
    "from collections import deque\n",
    "\n",
    "def ping():\n",
    "    !say done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Conf = Schema(dict(\n",
    "        eta=Num, min_eta=Num,\n",
    "        norm=Num,  accumsec=Num, norms=Dict({int: float}),  gradnorms=Dict({int: float}),\n",
    "        N=int, K=int, term={}, iter=int, epoch=int, dir=str,\n",
    "        C=even,  # window diameter; must be an even number\n",
    "        thresh=Num,  # gradient norm threshold for decreasing learning rate\n",
    "), extra=ALLOW_EXTRA, required=True)\n",
    "Conf = orig_type(Conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf = ut.AttrDict(\n",
    "    eta=.1, min_eta=.0001, norm=0, accumsec=0, norms={}, gradnorms={}, N=100,\n",
    "    C=4, K=6, iter=0, thresh=15, epoch=0,\n",
    "    term=dict(iters=None,\n",
    "              secs=10\n",
    "    ),\n",
    "    dir='cache',\n",
    ")\n",
    "cnf = Conf(cnf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bounds_check_window(1, toks, 4, len(toks))\n",
    "bounds_check_window_arr(0, toks, 4, len(toks))\n",
    "bounds_check_window(i, xs, winsize, N)\n",
    "\n",
    "s = sliding_window_jit_arr(samp_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(sliding_window(ls[:10], C=6))\n",
    "\n",
    "%timeit list(sliding_window(ls, C=4))\n",
    "%timeit list(sliding_window2(ls, C=4))\n",
    "%timeit list(sliding_window3(ls, C=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def grad_norm(Wsub):\n",
    "    \"\"\"Calculate norm of gradient, where first row\n",
    "    is input vector, rest are output vectors. For any row,\n",
    "    half of the entries are zeros, which allows a lot of\n",
    "    skipping for a faster computation\"\"\"\n",
    "    n = Wsub.shape[1] // 2\n",
    "    sm = 0\n",
    "    for i in xrange(n):\n",
    "        sm += Wsub[0, i] ** 2\n",
    "    for i in xrange(1, len(Wsub)):\n",
    "        for j in xrange(n, 2 * n):\n",
    "            sm += Wsub[i, j] ** 2\n",
    "    return np.sqrt(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grd = ns_grad_jit(Wsub)\n",
    "assert np.isclose(grad_norm(grd), np.linalg.norm(grad_norm(grd)))\n",
    "\n",
    "%timeit np.linalg.norm(grad_norm(grd))\n",
    "%timeit grad_norm(grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tks = np.array(all_text.split())\n",
    "# assert 'Albus_Dumbledore' in stoks\n",
    "# stoks, dropped = get_subsample(toks, thresh=THRESH)\n",
    "\n",
    "all_text = list(brown.words())\n",
    "dv = WordVectorizer().fit(all_text)\n",
    "toki = [dv.vocabulary_[x] for x in all_text]\n",
    "vc = dv.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = wut.init_w(len(vc), cnf.N, seed=1)\n",
    "We = W.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# with open('txt.txt','w') as f:\n",
    "#     f.write('\\n'.join(all_text))\n",
    "    \n",
    "\n",
    "!say done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = lambda: None\n",
    "a.ct, a.sw, a.ns, a.lr = (count(cf.iter),\n",
    "      sliding_window(iter_corpus, C=cf.C),\n",
    "      z.partition(cf.C, neg_sampler),\n",
    "      iter(learning_rates))\n",
    "for _ in xrange(cf.term['iters']):\n",
    "    next(a.ct)\n",
    "    next(a.sw)\n",
    "    next(a.ns)\n",
    "    next(a.lr)\n",
    "return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def grad_update_jit(W, sub_ixs, eta):\n",
    "    Wsub = W[sub_ixs]\n",
    "    grad = ns_grad_jit(Wsub)\n",
    "    gnorm = grad_norm(grad)\n",
    "    if gnorm > 5:  # clip gradient\n",
    "        grad /= gnorm\n",
    "    W[sub_ixs] = Wsub - eta * grad\n",
    "    \n",
    "def grad_update(W, sub_ixs, eta, ns_grad=ns_grad):\n",
    "    Wsub = W[sub_ixs]\n",
    "    grad = ns_grad(Wsub)\n",
    "    gnorm = grad_norm(grad)\n",
    "    if gnorm > 5:  # clip gradient\n",
    "        grad /= gnorm\n",
    "    W[sub_ixs] = Wsub - eta * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad_update_jit(W, ixa_, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(W=None, corp=None, cf={}, ns_grad=ns_grad, neg_sampler=None, vc=None, sliding_window=sliding_window):\n",
    "    # TODO: ensure neg samp != wi\n",
    "    if not os.path.exists(cf.dir):\n",
    "        os.mkdir(cf.dir)\n",
    "    st = time.time(); cf = Conf(cf)  #.copy()\n",
    "    norms = dict(cf.norms); gradnorms = dict(cf.gradnorms)\n",
    "    assert cf.N == W.shape[1] / 2, 'shape of W disagrees with conf'\n",
    "    maxnorms = deque([], 5)\n",
    "\n",
    "    normax = max(norms or [0]); gradnormax = max(gradnorms or [0]);\n",
    "    # global Win, Wout, w, cont, negsamp_lst, c, negsamps, sub_ixs\n",
    "    # Win, Wout = Cat.split(W)\n",
    "    iter_corpus = corp[cf.iter:]\n",
    "    learning_rates = np.linspace(cf.eta, cf.min_eta, len(iter_corpus))\n",
    "    assert neg_sampler is not None, \"Give me a negative sampler!\"\n",
    "    \n",
    "    iters_ = izip(count(cf.iter),\n",
    "                  sliding_window(iter_corpus, C=cf.C),\n",
    "                  z.partition(cf.C, neg_sampler),\n",
    "                  learning_rates,\n",
    "                 )\n",
    "    iters = ut.timeloop(iters_, **cf.term)\n",
    "    # W2 = W.copy()\n",
    "    for i, (w, cont_), negsamp_lst, eta in iters:\n",
    "        cont = [x for x in cont_ if x != w] if w in cont_ else cont_\n",
    "        for c, negsamps in izip(cont, negsamp_lst):\n",
    "#         for c, negsamps in zip(cont, negsamp_lst):\n",
    "            if (w in negsamps) or (c in negsamps):\n",
    "                negsamps = [x for x in negsamps if x not in {w, c}]\n",
    "\n",
    "            sub_ixs = np.array([w, c] + negsamps) # list(negsamps)\n",
    "            \n",
    "            grad_update_jit(W, sub_ixs, eta)\n",
    "#             grad_update(W2, sub_ixs, eta, ns_grad=ns_grad_jit)\n",
    "            \n",
    "#     assert np.allclose(W, W2)\n",
    "    tdur = time.time() - st\n",
    "    print('{:.2f} mins'.format(tdur / 60))\n",
    "    cf2 = update(cf, norms=norms, gradnorms=gradnorms, iter=i+1)\n",
    "    cf2['accumsec'] += tdur\n",
    "    if not cf2.term:\n",
    "        DataFrame(W, index=vc).to_csv(os.path.join(cf2.dir, 'n{}_e{}.csv'.format(cf.N, cf.epoch)))\n",
    "        cf2['epoch'] += 1\n",
    "        cf2 = update(cf2, iter=0)\n",
    "    else:\n",
    "        print(i, 'iters')\n",
    "    # ping()\n",
    "    return W, cf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -T lp5.txt -s -f sgd sgd(W=We.copy(), corp=toki, cf=update(cnf, term={'iters': 10000}), **fast_opts) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def concat_jit(arr, *xs):\n",
    "    X = len(xs)\n",
    "    A = len(arr)\n",
    "    a2 = np.empty(A + X, dtype=np.uint32)\n",
    "    for i in xrange(X):\n",
    "        a2[i] = xs[i]\n",
    "        \n",
    "    for i in xrange(X, A + X):\n",
    "        a2[i] = arr[i - X]\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negsamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_jit(negsamps, w, c) == np.array([w, c] + negsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit concat_jit(negsamps, w, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit concat_jit(negsamps, w, c)\n",
    "%timeit np.array([w, c] + negsamps)\n",
    "%timeit np.concatenate([[w, c], negsampsa])\n",
    "%timeit np.concatenate([[w, c], negsamps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.concatenate([[w, c], negsampsa]) == np.array([w, c] + negsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.concatenate([[w, c], negsampsa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negsampsa = np.array(negsamps)\n",
    "# %timeit np.array([w, c] + negsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngsamp = neg_sampler_j(toki, cnf.K)\n",
    "fast_opts = dict(ns_grad=ns_grad_jit, neg_sampler=ngsamp, sliding_window=sliding_window_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = sgd(W=We.copy(), corp=toki, cf=update(cnf, term={'iters': 10000}), **fast_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(w in negsamps) or (c in negsamps)\n",
    "set([w, c]) & set(negsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negsamps, w, c = [34096, 48494, 33883, 20513, 16756, 36496], 17591, 8187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def ix_jit(W, ixs):\n",
    "    return W[ixs]\n",
    "\n",
    "def ix_jit_l2(W, ixs: [int]):\n",
    "    return ix_jit(W, np.array(ixs))\n",
    "\n",
    "@nopython\n",
    "def ix_jit_l(W, ixs: [int]):\n",
    "    n = len(ixs)\n",
    "    ixs_a = np.empty(n, dtype=np.int32)\n",
    "    for i in xrange(n):\n",
    "        ixs_a[i] = ixs[i]\n",
    "    return W[ixs_a]\n",
    "\n",
    "@nopython\n",
    "def ix_jit2(W, ixs):\n",
    "    m, n = W.shape\n",
    "    I = len(ixs)\n",
    "    res = np.empty((I, n))\n",
    "    for i in xrange(I):\n",
    "        for j in xrange(n):\n",
    "            res[i, j] = W[ixs[i], j]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ix_jit_l(W, list(ixs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ixsles = map(list, nr.randint(0, len(W), size=(100000, 10)))\n",
    "ixsaes = nr.randint(0, len(W), size=(100000, 10))\n",
    "ixl = ixses[0]\n",
    "ixa = np.array(ixl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from py.test import raises\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def same_ix(f, ix):\n",
    "    assert (W[ix] == f(W, ix)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "same_ix(ix_jit, ixa)\n",
    "with raises(numba.TypingError):\n",
    "    same_ix(ix_jit, ixl)\n",
    "\n",
    "same_ix(ix_jit2, ixa)\n",
    "same_ix(ix_jit2, ixl)\n",
    "same_ix(ix_jit_l, ixl)\n",
    "same_ix(ix_jit_l2, ixl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bench_ix(ixa, ixl):\n",
    "    W[ixa]\n",
    "    W[ixl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert (W[ixa] == ix_jit(W, ixa)).all()\n",
    "assert (W[ixa] == ix_jit2(W, ixa)).all()\n",
    "assert (W[ixa] == ix_jit2(W, ixl)).all()\n",
    "assert (W[ixa] == ix_jit_l(W, ixl)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time for ixl_ in ixsles: 1\n",
    "%time for ixa_ in ixsaesl: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time for ixl_ in ixsles: W[ixl_]\n",
    "%time for ixa_ in ixsaesl: W[ixa_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time for ixl_ in ixses: ix_jit2(W, ixl_)\n",
    "%time for ixl_ in ixses: ix_jit_l(W, ixl_)\n",
    "%time for ixl_ in ixses: W[ixl_]\n",
    "%time for ixl_ in ixses: W[ixl_]\n",
    "%time for ixl_ in ixses: ix_jit_l2(W, ixl_)\n",
    "%time for ixl_ in ixses: ix_jit(W, np.array(ixl_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit W[ixs]\n",
    "%timeit ix_jit(W, ixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd(W=We.copy(), corp=toki, cf=update(cnfe, term={'iters': 10000}), **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -T lp.txt -s -f sgd sgd(W=We.copy(), corp=toki, cf=update(cnfe, term={'iters': 10000})) # ls[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rand_ixs = lambda W, n=8, axis=0: nr.randint(0, W.shape[axis], size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ixs = [ 1, 4, 7, 99, 486, 263, 924]\n",
    "w1, w2 = wtst.copy(), wtst.copy()\n",
    "# w1, w2 = wtst[ixs].copy(), wtst[ixs].copy()\n",
    "gr = ns_grad(w1[ixs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit ineg(w1, ixs, gr)\n",
    "%timeit inegp(w2, ixs, gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnfe = update(cnf, C=4, iter=0, term=dict(), N=100,  dir='cache/v12', epoch=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print('Epoch {}'.format(cnfe.epoch))\n",
    "    We2, cnfe = sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()), **fast_opts)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ilen = lambda xs: sum(1 for _ in xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk; reload(nltk)\n",
    "# , reuters\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ilen(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ilen(reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnfe = update(cnf, C=4, iter=0, term=dict(), N=100, dir='cache/v12', epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default\n",
    "\n",
    "size=100\n",
    "alpha=0.025\n",
    "window=5\n",
    "sample=0\n",
    "negative=0\n",
    "sg=1\n",
    "iter=4\n",
    "\n",
    "min_alpha=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gparams = dict(\n",
    "    size=120, # 80, #\n",
    "    alpha=0.025,\n",
    "    window=2,\n",
    "    sample=0,\n",
    "    negative=2,  #[5, 7, 10, 12, 15, 17], 0\n",
    "    sg=1,\n",
    "    iter=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ev(mod):\n",
    "    ans = mod.accuracy('src/questions-words.txt', restrict_vocab=10000)\n",
    "    sect = [d for d in ans if d['section'] == 'total']\n",
    "    return sum([1 for d in sect for _ in d['correct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time gmod = Word2Vec(brown.sents(), **update(gparams))\n",
    "ev(gmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def to_ints(wds):\n",
    "    le = LabelEncoder().fit(wds)\n",
    "    ints = le.transform(wds)\n",
    "    return list(ints), le\n",
    "\n",
    "def to_list_gen(f):\n",
    "    @wraps(f)\n",
    "    def f2(*a, **kw):\n",
    "        gen = f(*a, **kw)\n",
    "        return (list(x) for x in gen)\n",
    "    return f2\n",
    "\n",
    "def tokenize(wds):\n",
    "    alpha_re = re.compile(r'[A-Za-z]')\n",
    "    return [w for w in wds if alpha_re.search(w)]\n",
    "\n",
    "def prune_words(wds, keep_n_words=30000, min_counts=None):\n",
    "    if (keep_n_words is None) and (min_counts is None):\n",
    "        return wds\n",
    "    cts = Counter(wds)\n",
    "    if min_counts is not None:\n",
    "        return [w for w in wds if cts[w] >= min_counts]\n",
    "    elif keep_n_words is not None:\n",
    "        keeps = set(sorted(cts, key=cts.get, reverse=True)[:keep_n_words])\n",
    "        \n",
    "    return [w for w in wds if w in keeps]\n",
    "\n",
    "# c2 = prune_words(brown.words(), keep_n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toks, le = to_ints(nopunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def from_gensim_params(params, cnf, **upkw):\n",
    "    gsim2param_names = dict(negative='K', alpha='eta', size='N')\n",
    "    newparams = {pn: params[gn] for gn, pn in gsim2param_names.items()}\n",
    "    newparams['C'] = params['window'] * 2\n",
    "    cnf2 = update(cnf, **newparams, **upkw)\n",
    "    return cnf2\n",
    "\n",
    "cnff = Conf(from_gensim_params(gparams, cnfe, dir='cache/v13'))\n",
    "# cnff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = gparams\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class word2vec(object):\n",
    "    def __init__(self, words, cnf, neg_sampler=to_list_gen(neg_sampler_np),\n",
    "                 keep_n_words=None, min_counts=None, **sgd_kwds):\n",
    "        text = prune_words(tokenize(words), keep_n_words=keep_n_words, min_counts=min_counts)\n",
    "        \n",
    "        self.text = text\n",
    "        self.toks, self.le = to_ints(self.text)\n",
    "        self.cnf = update(cnf, term={})\n",
    "        self.W = wut.init_w(len(self.le.classes_), cnf.N)\n",
    "        self.sgd_kwds = sgd_kwds\n",
    "        self.neg_sampler = neg_sampler(self.toks, cnf.K)\n",
    "        \n",
    "    def run(self, term=None, **sgd_kwds):\n",
    "        cnf = self.cnf\n",
    "        if term:\n",
    "            cnf = update(cnf, term=term)\n",
    "        res = sgd(W=self.W.copy(), corp=self.toks, neg_sampler=self.neg_sampler,\n",
    "                  cf=cnf, vc=self.le.classes_, **z.merge(self.sgd_kwds, sgd_kwds))\n",
    "        self.W, self.cnf = res\n",
    "        \n",
    "    @property\n",
    "    def df(self):\n",
    "        return DataFrame(self.W.copy(), index=self.le.classes_)\n",
    "\n",
    "modf = word2vec(brown.words(), cnff, neg_sampler=neg_sampler_j, keep_n_words=None, min_counts=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('cache/txt.txt','w') as f:\n",
    "    f.write('\\n'.join(modf.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_opts = dict(ns_grad=ns_grad_jit, sliding_window=sliding_window_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(4):   \n",
    "    modf.run(term={}, **fast_opts)  # 'iters': 10000\n",
    "!say done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modf.run(term={}, **fast_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modf.cnf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def partition(pred, iterable):\n",
    "    'Use a predicate to partition entries into false entries and true entries'\n",
    "    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9\n",
    "    t1, t2 = it.tee(iterable)\n",
    "    return it.filterfalse(pred, t1), ifilter(pred, t2)\n",
    "\n",
    "with open('src/questions-words.txt', 'r') as f:\n",
    "    qlns = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('src/questions-words.txt', 'r') as f:\n",
    "    qlns = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modf.df = DataFrame(modf.W.copy(), index=modf.le.classes_)\n",
    "W2 = modf.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sections, qs = partition(lambda s: not s.startswith(':'), qlns)\n",
    "qs = list(qs)\n",
    "# allwds = set(modf.df.index)\n",
    "# del allwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cts = Series(Counter(modf.text)) # / len(modf.text)\n",
    "assert (cts.index == modf.le.classes_).all()\n",
    "keeps = cts >= 5\n",
    "Wk = modf.df.divide(norm(modf.df, axis=1), axis=0)\n",
    "Wsmall = Wk[keeps]\n",
    "assert (norm(Wk, axis=1).round(4) == 1).all(), 'Not normalized'\n",
    "# assert (Wk.sum(axis=1).round(4) == 1).all(), 'Not normalized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -s -f eval eval_qs(Wk, lim=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_vec(w, W):\n",
    "    if isinstance(w, str):\n",
    "        return W.ix[w]\n",
    "    return w\n",
    "\n",
    "def to_vec2(w, W, wd2row=None):\n",
    "    if isinstance(w, str):\n",
    "        return W.values[wd2row[w]]\n",
    "    return w\n",
    "\n",
    "neg = lambda x: -x\n",
    "\n",
    "def combine(plus=[], minus=[], W=None, wd2row=None):\n",
    "    to_vec_ = partial(to_vec, W=W)\n",
    "    vecs = map(to_vec_, plus) + map(z.comp(neg, to_vec_), minus)\n",
    "    v = sum(vecs) / len(vecs)\n",
    "    return v / norm(v)\n",
    "\n",
    "def combine2(plus=[], minus=[], W=None, wd2row=None):\n",
    "    # plus_ix = [wd2row[p] for p in plus]\n",
    "    # minus_ix = [wd2row[p] for p in minus]\n",
    "    ixs = [wd2row[p] for p in plus + minus]\n",
    "    vecs1 = Wk.values[ixs]\n",
    "    to_vec_ = partial(to_vec2, W=W, wd2row=wd2row)\n",
    "    \n",
    "    vecs = map(to_vec_, plus) + map(z.comp(neg, to_vec_), minus)\n",
    "    vecs = np.array(vecs)\n",
    "    return combine_(vecs)\n",
    "    v = sum(vecs) / len(vecs)\n",
    "    return v / norm(v)\n",
    "\n",
    "\n",
    "@nopython\n",
    "def combine_(vecs):\n",
    "#     v = sum(vecs) / len(vecs)\n",
    "    v = np.sum(vecs, axis=0) / len(vecs)\n",
    "    return v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = np.array(vecs)\n",
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -s -f combine2 combine2(plus=[b, c], minus=[a], W=Wk, wd2row=wd2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wvec = combine(plus=[b, c], minus=[a], W=Wk)\n",
    "wvec[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mk_wd2row = lambda W: dict(zip(W.index, count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wd2ix = mk_wd2row(Wk)\n",
    "wvec2 = \n",
    "wvec2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit combine(plus=[b, c], minus=[a], W=Wk)\n",
    "%timeit combine2(plus=[b, c], minus=[a], W=Wk, wd2row=wd2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine2(plus=[b, c], minus=[a], W=Wk, wd2row=wd2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(wvec == wvec2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@nopython\n",
    "def index(wd2ix):\n",
    "    \n",
    "    return wd2ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wds, arr, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index({'abc': 123})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wv = Wk.values\n",
    "f1 = lambda: Wk.ix['youngster']\n",
    "f2 = lambda: Wk.iloc[-18]\n",
    "f3 = lambda: Wv[-18]\n",
    "assert np.allclose(f1(), f3())\n",
    "assert np.allclose(f1(), f2())\n",
    "\n",
    "%timeit f1()\n",
    "%timeit f2()\n",
    "%timeit f3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval(q, Wall=None, Wsmall=None, Wnorm=None, allwds=None):\n",
    "    qwds = a, b, c, d = q.split()\n",
    "    if allwds is None:\n",
    "        print('Warning: precalculate `allwds`')\n",
    "        allwds = set(Wall.index)\n",
    "    missing_wds = {a, b, c} - allwds\n",
    "    if missing_wds:\n",
    "        # print(u'\\u2639', end='')\n",
    "#         print(u'.', end='')\n",
    "        return False\n",
    "    wvec = combine(plus=[b, c], minus=[a], W=Wall)\n",
    "    [closest] = wut.get_closestn(wd=wvec, W=Wsmall, n=1, exclude=[a, b, c], just_word=1)\n",
    "    ans = closest == d\n",
    "        \n",
    "    return ans\n",
    "\n",
    "        \n",
    "def eval_qs(Wsmall, Wall, lim=None):\n",
    "    Wn = np.linalg.norm(Wsmall, axis=1)\n",
    "    allwds = set(Wall.index)\n",
    "    sm = 0\n",
    "    for ql in qs[:lim]:\n",
    "        print(list(allwds)[:5])\n",
    "        e = eval(ql, Wall=Wall, Wsmall=Wsmall, Wnorm=Wn, allwds=allwds)\n",
    "        if e:\n",
    "#             print('!', end='')\n",
    "            print(ql, end=':')\n",
    "            sm += 1\n",
    "    return sm\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wk.ix??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wk._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %lprun -s -f eval eval_qs(Wk, lim=500)\n",
    "%lprun -s -f combine eval(ql, Wall=Wk, Wsmall=Wk, Wnorm=norm(Wk, axis=1), allwds=set(Wk.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %lprun eval(ql, Wall=Wk, Wsmall=Wk, Wnorm=norm(Wk, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = eval_qs(Wk, Wk, lim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod.syn0norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b, c, d = 'boy', 'girl', 'brothers', 'sisters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = np.linalg.norm\n",
    "reload(wut);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "closests = wut.get_closestn.closests\n",
    "print(len(closests))\n",
    "closests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm(Wk, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wvec = combine(plus=[b, c], minus=[a], W=Wk)\n",
    "cl = wut.get_closestn(wd=wvec, W=Wk, Wnorm=None, n=1, exclude=[a, b, c], just_word=1)\n",
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wut.cdist(wvec, Wk)[Wk.index.isin([a, b, c])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sis = Wsmall.ix['sisters']\n",
    "(sis @ wvec) / (norm(sis) * norm(wvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ga, gb, gc, gd = gmod.vocab[a], gmod.vocab[b], gmod.vocab[c], gmod.vocab[d],\n",
    "ga, gb, gc, gd = gmod.syn0norm[ga.index], gmod.syn0norm[gb.index], gmod.syn0norm[gc.index], gmod.syn0norm[gd.index]\n",
    "gv = (gb + gc - ga) / 3\n",
    "gv /= norm(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norms = gmod.syn0norm @ gv\n",
    "isort = np.argsort(norms)[::-1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[gmod.index2word[i] for i in isort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norms[isort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod.syn0norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_closests?\n",
    "get_closests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(Wk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsc = gmod.accuracy('src/questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsc_dct = {doc.pop('section'): doc for doc in gsc}\n",
    "gfound = {w for set_ in gsc_dct['total']['correct'] for w in set_[:3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b, c, gmod.most_similar(positive=[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod.most_similar(positive=[b, c], negative=[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsc_dct['family']['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in gsc:\n",
    "    print(doc['section'], end=': ')\n",
    "    print(len(doc['correct']))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(gsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnfs = update(cnfe, dir='cache/slow')\n",
    "mod = word2vec(brown.words(), cnfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time mod.run(term={})  # 'iters': 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gensim benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim_sents = brown.sents() # [s.orth_.split() for s in atks.sents]\n",
    "\n",
    "param_vals = dict(\n",
    "    sample=[1e-3, 5e-3, 1e-2, 5e-2, .1, .15, 0],\n",
    "    negative=range(0, 8),  #[5, 7, 10, 12, 15, 17],\n",
    "    window=range(2, 20),\n",
    "    sg=[0, 1],\n",
    "    size=np.arange(1, 40) * 4,\n",
    "    alpha=[0.05, 0.025, 0.01, 0.005],\n",
    "    iter=range(1, 5),\n",
    "    \n",
    ")\n",
    "\n",
    "param_lst = ['alpha', 'iter', 'negative', 'sample', 'sg', 'size', 'window']\n",
    "assert sorted(param_vals) == param_lst, 'Need to update param_lst'\n",
    "\n",
    "cs = param_lst + ['score']\n",
    "to_param_dct = lambda xs: OrderedDict(zip(param_lst, xs))\n",
    "from_param_dct = lambda dct, cs=param_lst: [dct[c] for c in cs]\n",
    "\n",
    "# param_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_closests(to_param_dct([0.025, 4.0, 0.0, 0.0, 1.0, 80.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_param_dct([0.025, 4.0, 0.0, 0.0, 1.0, 80.0, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_closest(k, v, poss=param_vals):\n",
    "    return min(poss[k], key=lambda x: abs(v - x))\n",
    "\n",
    "def get_closests(dct, poss=param_vals):\n",
    "    return {k: get_closest(k, v, poss=poss) for k, v in dct.items()}\n",
    "\n",
    "def param_gen(params, n_iters=None):\n",
    "    counter = count() if n_iters is None else range(n_iters)\n",
    "    for i in counter:\n",
    "        yield {k: nr.choice(v) for k, v in param_vals.items()}\n",
    "\n",
    "def run_model(n=None, perfs=None, lfname='cache/log.csv'):\n",
    "    pg = param_gen(param_vals, n)\n",
    "\n",
    "    for params in pg:\n",
    "        st = time.time()\n",
    "        model = Word2Vec(sentences=gensim_sents, workers=4, **params)\n",
    "        perf = dict(params)\n",
    "        perf['score'] = score(model)\n",
    "        perfs.append(perf)\n",
    "        print('time: {:.2f}'.format(time.time() - st))\n",
    "      \n",
    "        if not os.path.exists(lfname):\n",
    "            mode, header = 'w', True\n",
    "        else:\n",
    "            mode, header = 'a', False\n",
    "        with open(lfname, mode) as f:\n",
    "            DataFrame([perf])[cs].to_csv(f, header=header, sep='\\t')\n",
    "        sys.stdout.flush()\n",
    "    return perfs\n",
    "\n",
    "def score(model, restrict_vocab=10000):\n",
    "    acc = model.accuracy('src/questions-words.txt', restrict_vocab=restrict_vocab)\n",
    "    [tot] = [d for d in acc if d['section'] == 'total']\n",
    "    print(len(tot['correct']), '/', len(tot['incorrect']) + len(tot['correct']), end=' ')\n",
    "    return len(tot['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_ex = next(param_gen(param_vals, None))\n",
    "ex_vals = map(itg(1), sorted(param_ex.items()))\n",
    "to_param_dct(ex_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = minimize(rosen, x0, method='nelder-mead',\n",
    "...                options={'xtol': 1e-8, 'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_func(paramlist):\n",
    "    print(paramlist)\n",
    "    param_dct_ = to_param_dct(paramlist)\n",
    "    param_dct = {k: get_closest(k, v) for k, v in param_dct_.items()}\n",
    "    print(param_dct)\n",
    "    model = Word2Vec(sentences=gensim_sents, workers=4, **param_dct)\n",
    "    return score(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perfs = []\n",
    "perfs = run_model(n=None, perfs=perfs)\n",
    "## End Gensim benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perfs[:4][cs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Series(list(cts.values())).value_counts(normalize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modf.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z.merge({2: 4}, {2: 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(**kw):\n",
    "    print(kw)\n",
    "    \n",
    "f(a=2, **{'a': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  # neg_sampler=ngsamp, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngsamp = neg_sampler_j(toki, cnfe.K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%lprun -T lp5.txt -s -f sgd sgd(W=We.copy(), corp=toki, cf=update(cnfe, term={'iters': 10000}), **fast_opts) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnfe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmod.most_similar('politician')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()), **fast_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.allclose(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ineg(w1, ixs, gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inegp(w2, ixs, gr)\n",
    "np.allclose(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtst[ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtst = wut.init_w(1000, 50)\n",
    "wsub = wtst[:8].copy()\n",
    "gr = ns_grad(wsub)\n",
    "# gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ineg1(wsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad, grad2 = grd, grd2\n",
    "Wsub = W2[[w, c] + list(negsamps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(approx_grad(Wsub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(Wsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(ns_grad(W2[[w, c] + list(negsamps)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(vc.__getitem__, [c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(vc.__getitem__, negsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negsamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del w, cont, c, negsamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(grad, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc[19276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toki[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgdpart = partial(sgd, W=W.copy(), corp=toki, cf=update(cnfe, term={'iters': 1}))\n",
    "_ = sgdpart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradnorm_vec = np.linalg.norm(grad, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if np.linalg.norm(gradnorm_vec) > 5:\n",
    "    grad = grad / gradnorm_vec[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradnormed = np.divide(grad, gradnorm)\n",
    "gradnormed = grad / gradnorm[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(gradnormed, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradnormed.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "7.12\n",
    "  1085718\n",
    "6.00742\n",
    "-> 684739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd(W=W.copy(), corp=toki, cf=update(cnf, term={'iters': 1}));\n",
    "%prun -qD prof.txt sgd(W=W.copy(), corp=toki, cf=update(cnf, term={'iters': 5000})) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We = pd.read_csv('cache/v9/n15_e26.csv', index_col=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(*zip(*gns), alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print('Epoch {}'.format(cnfe.epoch))\n",
    "    We2, cnfe = sgd(W=We.copy(), corp=toki, cf=update(cnfe, term=dict()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "We2 - We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "W2, cnf2 = sgd(W=W.copy(), corp=toki, cf=update(cnf, term={'mins': 60})) # ls[:20]\n",
    "sw = sliding_window(toki, C=cnf.C)\n",
    "# 799509 iters\n",
    "# CPU times: user 12min 29s, sys: 4.47 s, total: 12min 34s\n",
    "# Wall time: 12min 35s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "W3, cnf3 = sgd(W=W2.copy(), corp=toki, cf=update(cnf2, iter=0)) # ls[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "W3, cnf2 = sgd(W=W2.copy(), corp=toki, cf=update(cnf, term={'mins': 5})) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W3, cnf3 = sgd(W=W3.copy(), corp=toki, cf=update(cnf3, term={'mins': 5}), ) # ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wd = DataFrame(W2, index=vc)\n",
    "wd.to_csv('cache/v2/n6_e1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd.mean(axis=1).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wd[10:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Series(cnfe.norms).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, x in enumerate([-1, 1,6,8]):\n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf2.norms[48000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "gnrms = Series(cnfe.gradnorms)\n",
    "gnrms.plot()\n",
    "pd.rolling_mean(gnrms, 20).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update equation\n",
    "$$\n",
    "    \\frac{\\partial E}\n",
    "         {\\partial \\boldsymbol v_{w_j}' ^T \\boldsymbol h}\n",
    "         = \\sigma(\\boldsymbol v_{w_j}' ^T \\boldsymbol h) -t_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyze word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
